{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Deploying OpenShift on IBM Power Systems This Github organization is home to Infrastructure as Code (IaC) patterns to help deploy OpenShift on IBM Power Systems. Terraform is used to create the infrastructure and ansible playbooks are used to deploy and customize OpenShift. The following diagram shows the high level overview of the IaC pattern:","title":"Home"},{"location":"#deploying-openshift-on-ibm-power-systems","text":"This Github organization is home to Infrastructure as Code (IaC) patterns to help deploy OpenShift on IBM Power Systems. Terraform is used to create the infrastructure and ansible playbooks are used to deploy and customize OpenShift. The following diagram shows the high level overview of the IaC pattern:","title":"Deploying OpenShift on IBM Power Systems"},{"location":"ocp4-upi-kvm/","text":"Table of Contents Table of Contents Introduction Automation Host Prerequisites Libvirt Prerequisites OCP Install Contributing Introduction The ocp4-upi-kvm project provides Terraform based automation code to help the deployment of OpenShift Container Platform (OCP) 4.x on KVM VMs using libvirt. This project leverages the following ansible playbook to setup a helper node (bastion) for OCP deployment. :heavy_exclamation_mark: For bugs/enhancement requests etc. please open a GitHub issue :information_source: The main branch must be used with latest OCP pre-release versions only. For stable releases please checkout specific release branches - { release-4.5 , release-4.6 ...} and follow the docs in the specific release branches. Automation Host Prerequisites The automation needs to run from a system with internet access. This could be your laptop or a VM with public internet connectivity. This automation code has been tested on the following 64-bit Operating Systems: - Linux ( preferred ) - Mac OSX (Darwin) Follow the guide to complete the prerequisites. Libvirt Prerequisites Follow the guide to complete the Libvirt prerequisites. OCP Install Follow the quickstart guide for OCP installation on KVM using libvirt. Contributing Please see the contributing doc for more details. PRs are most welcome !!","title":"Deploying OpenShift on KVM"},{"location":"ocp4-upi-kvm/#table-of-contents","text":"Table of Contents Introduction Automation Host Prerequisites Libvirt Prerequisites OCP Install Contributing","title":"Table of Contents"},{"location":"ocp4-upi-kvm/#introduction","text":"The ocp4-upi-kvm project provides Terraform based automation code to help the deployment of OpenShift Container Platform (OCP) 4.x on KVM VMs using libvirt. This project leverages the following ansible playbook to setup a helper node (bastion) for OCP deployment. :heavy_exclamation_mark: For bugs/enhancement requests etc. please open a GitHub issue :information_source: The main branch must be used with latest OCP pre-release versions only. For stable releases please checkout specific release branches - { release-4.5 , release-4.6 ...} and follow the docs in the specific release branches.","title":"Introduction"},{"location":"ocp4-upi-kvm/#automation-host-prerequisites","text":"The automation needs to run from a system with internet access. This could be your laptop or a VM with public internet connectivity. This automation code has been tested on the following 64-bit Operating Systems: - Linux ( preferred ) - Mac OSX (Darwin) Follow the guide to complete the prerequisites.","title":"Automation Host Prerequisites"},{"location":"ocp4-upi-kvm/#libvirt-prerequisites","text":"Follow the guide to complete the Libvirt prerequisites.","title":"Libvirt Prerequisites"},{"location":"ocp4-upi-kvm/#ocp-install","text":"Follow the quickstart guide for OCP installation on KVM using libvirt.","title":"OCP Install"},{"location":"ocp4-upi-kvm/#contributing","text":"Please see the contributing doc for more details. PRs are most welcome !!","title":"Contributing"},{"location":"ocp4-upi-kvm/CODE_OF_CONDUCT/","text":"Contributor Covenant Code of Conduct Our Pledge We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. Our Standards Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Enforcement Responsibilities Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate. Scope This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement. Contact with repository owners. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident. Enforcement Guidelines Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct: 1. Correction Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested. 2. Warning Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban. 3. Temporary Ban Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban. 4. Permanent Ban Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html. Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder .","title":"Contributor Covenant Code of Conduct"},{"location":"ocp4-upi-kvm/CODE_OF_CONDUCT/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"ocp4-upi-kvm/CODE_OF_CONDUCT/#our-pledge","text":"We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.","title":"Our Pledge"},{"location":"ocp4-upi-kvm/CODE_OF_CONDUCT/#our-standards","text":"Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"ocp4-upi-kvm/CODE_OF_CONDUCT/#enforcement-responsibilities","text":"Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.","title":"Enforcement Responsibilities"},{"location":"ocp4-upi-kvm/CODE_OF_CONDUCT/#scope","text":"This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.","title":"Scope"},{"location":"ocp4-upi-kvm/CODE_OF_CONDUCT/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement. Contact with repository owners. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident.","title":"Enforcement"},{"location":"ocp4-upi-kvm/CODE_OF_CONDUCT/#enforcement-guidelines","text":"Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:","title":"Enforcement Guidelines"},{"location":"ocp4-upi-kvm/CODE_OF_CONDUCT/#1-correction","text":"Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.","title":"1. Correction"},{"location":"ocp4-upi-kvm/CODE_OF_CONDUCT/#2-warning","text":"Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.","title":"2. Warning"},{"location":"ocp4-upi-kvm/CODE_OF_CONDUCT/#3-temporary-ban","text":"Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.","title":"3. Temporary Ban"},{"location":"ocp4-upi-kvm/CODE_OF_CONDUCT/#4-permanent-ban","text":"Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community.","title":"4. Permanent Ban"},{"location":"ocp4-upi-kvm/CODE_OF_CONDUCT/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html. Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder .","title":"Attribution"},{"location":"ocp4-upi-kvm/CONTRIBUTING/","text":"Contributing This project is Apache 2.0 Licenced and welcomes external contributions. When contributing to this repository, please first discuss the change you wish to make via an issue . Please note we have a code of conduct , please follow it in all your interactions with the project. Issues If you find any issue with the code or documentation please submit an issue . It is best to check out existing issues first to see if a similar one is open or has already been discussed. Pull Request Process To contribute code or documentation, please submit a pull request . Always take the latest update from upstream/master before creating a pull request. Ensure your changes work fine and have no syntax problems. Also, verify that it does not break the existing code flow. Update the README.md or relevant documents with details of changes to the code. This includes variables change, added or updated feature, change in steps, dependencies change, etc. Make use of proper commit message. Mention the issue# which you are planning to address eg: Fixes #38. After creating the pull request ensure you implement all the review comments given if any. Pull request will be merged only when it has at least two approvals from the list of reviewers. Please read Developer Certificate of Origin and sign-off your commit using command git commit -s . Spec Formatting Conventions Documents in this repository will adhere to the following rules: Lines are wrapped at 80 columns (when possible) Use spaces to indent your code. Do not use tab character, instead can use 2/4 spaces.","title":"Contributing"},{"location":"ocp4-upi-kvm/CONTRIBUTING/#contributing","text":"This project is Apache 2.0 Licenced and welcomes external contributions. When contributing to this repository, please first discuss the change you wish to make via an issue . Please note we have a code of conduct , please follow it in all your interactions with the project.","title":"Contributing"},{"location":"ocp4-upi-kvm/CONTRIBUTING/#issues","text":"If you find any issue with the code or documentation please submit an issue . It is best to check out existing issues first to see if a similar one is open or has already been discussed.","title":"Issues"},{"location":"ocp4-upi-kvm/CONTRIBUTING/#pull-request-process","text":"To contribute code or documentation, please submit a pull request . Always take the latest update from upstream/master before creating a pull request. Ensure your changes work fine and have no syntax problems. Also, verify that it does not break the existing code flow. Update the README.md or relevant documents with details of changes to the code. This includes variables change, added or updated feature, change in steps, dependencies change, etc. Make use of proper commit message. Mention the issue# which you are planning to address eg: Fixes #38. After creating the pull request ensure you implement all the review comments given if any. Pull request will be merged only when it has at least two approvals from the list of reviewers. Please read Developer Certificate of Origin and sign-off your commit using command git commit -s .","title":"Pull Request Process"},{"location":"ocp4-upi-kvm/CONTRIBUTING/#spec-formatting-conventions","text":"Documents in this repository will adhere to the following rules: Lines are wrapped at 80 columns (when possible) Use spaces to indent your code. Do not use tab character, instead can use 2/4 spaces.","title":"Spec Formatting Conventions"},{"location":"ocp4-upi-kvm/docs/automation_host_prereqs/","text":"Automation Host Prerequisites Automation Host Prerequisites Configure Your Firewall Automation Host Setup Terraform Terraform Providers Git RHCOS and RHEL 8.X Images for OpenShift Download the RHEL Qcow2 image Customize the RHEL Qcow2 image Download the RHCOS Qcow2 image Configure Your Firewall If your automation host is behind a firewall, you will need to ensure the following ports are open in order to use ssh, http, and https: - 22, 443, 80 These additional ports are required for the ocp cli ( oc ) post-install: - 6443 Automation Host Setup Install the following packages on the automation host. Select the appropriate install binaries based on your automation host platform - Mac/Linux. It's preferable to run this automation code on Linux host since Linux is required for customizing the RHEL image. Terraform Terraform >= 0.13.0 : Please refer to the link for instructions on installing Terraform. For validating the version run terraform version command after install. Terraform binary on IBM Power ( ppc64le ) is not available for download from Hashicorp website. You can download it from the following link-1 or link-2 or you will need to compile it from source by running: TAG_VERSION=v0.13.5 git clone https://github.com/hashicorp/terraform --branch $TAG $GOPATH/src/github.com/hashicorp/terraform cd $GOPATH/src/github.com/hashicorp/terraform TF_DEV=1 scripts/build.sh Validate using: $GOPATH/bin/terraform version Terraform Providers Please follow the guide to build and install the required Terraform providers. Git Git : Please refer to the link for instructions on installing Git. RHCOS and RHEL 8.X Images for OpenShift You'll need to have the RedHat CoreOS (RHCOS) and RHEL 8.2 (or later) image available on the automation host or via an HTTP(S) server. RHEL 8.x image is used by bastion node, and RHCOS image is used for boostrap, master and worker nodes. Download the RHEL Qcow2 image Login to the RedHat portal and click on the Downloads tab. Click on the product Red Hat Enterprise Linux 8 from the list. In the \"Product Variant\" drop-down select \"Red Hat Enterprise Linux for Power, little endian\". In the \"Version\" drop-down select the version of RHEL you want to download. For example, will use the latest as 8.2. In the \"Product Software\" tab, click on \"Download Now\" button adjacent to \"Red Hat Enterprise Linux 8.2 Update KVM Guest Image\". Save the Qcow2 image. If your automation host is not Linux, then you will need to copy the Qcow2 image to a Linux host for the next step. Customize the RHEL Qcow2 image Customize the Qcow2 image to set a root password and disable the cloud-init service. You will need the 'libguestfs-tools' package installed on the Linux machine to run the below command (Not available on Mac/Windows). # virt-customize -a <qcow2 image file name> --root-password password:<password> --uninstall cloud-init [ 0.0] Examining the guest ... [ 11.5] Setting a random seed [ 11.5] Uninstalling packages: cloud-init [ 13.9] Setting passwords [ 15.6] Finishing off On successful completion, copy the image back to the automation host (if it was not Linux) or make it available via an HTTP(S) server. Update the following Terraform input variables: * rhel_password = \"<password>\" Download the RHCOS Qcow2 image Select the RHCOS version you need from the OpenShift mirror . Find and download the QEMU qcow2 image gzip file. eg: rhcos-4.4.9-ppc64le-qemu.ppc64le.qcow2.gz Extract the Qcow2 image and place it on the automation host or make it available via an HTTP(S) server.","title":"Automation Host Prerequisites"},{"location":"ocp4-upi-kvm/docs/automation_host_prereqs/#automation-host-prerequisites","text":"Automation Host Prerequisites Configure Your Firewall Automation Host Setup Terraform Terraform Providers Git RHCOS and RHEL 8.X Images for OpenShift Download the RHEL Qcow2 image Customize the RHEL Qcow2 image Download the RHCOS Qcow2 image","title":"Automation Host Prerequisites"},{"location":"ocp4-upi-kvm/docs/automation_host_prereqs/#configure-your-firewall","text":"If your automation host is behind a firewall, you will need to ensure the following ports are open in order to use ssh, http, and https: - 22, 443, 80 These additional ports are required for the ocp cli ( oc ) post-install: - 6443","title":"Configure Your Firewall"},{"location":"ocp4-upi-kvm/docs/automation_host_prereqs/#automation-host-setup","text":"Install the following packages on the automation host. Select the appropriate install binaries based on your automation host platform - Mac/Linux. It's preferable to run this automation code on Linux host since Linux is required for customizing the RHEL image.","title":"Automation Host Setup"},{"location":"ocp4-upi-kvm/docs/automation_host_prereqs/#terraform","text":"Terraform >= 0.13.0 : Please refer to the link for instructions on installing Terraform. For validating the version run terraform version command after install. Terraform binary on IBM Power ( ppc64le ) is not available for download from Hashicorp website. You can download it from the following link-1 or link-2 or you will need to compile it from source by running: TAG_VERSION=v0.13.5 git clone https://github.com/hashicorp/terraform --branch $TAG $GOPATH/src/github.com/hashicorp/terraform cd $GOPATH/src/github.com/hashicorp/terraform TF_DEV=1 scripts/build.sh Validate using: $GOPATH/bin/terraform version","title":"Terraform"},{"location":"ocp4-upi-kvm/docs/automation_host_prereqs/#terraform-providers","text":"Please follow the guide to build and install the required Terraform providers.","title":"Terraform Providers"},{"location":"ocp4-upi-kvm/docs/automation_host_prereqs/#git","text":"Git : Please refer to the link for instructions on installing Git.","title":"Git"},{"location":"ocp4-upi-kvm/docs/automation_host_prereqs/#rhcos-and-rhel-8x-images-for-openshift","text":"You'll need to have the RedHat CoreOS (RHCOS) and RHEL 8.2 (or later) image available on the automation host or via an HTTP(S) server. RHEL 8.x image is used by bastion node, and RHCOS image is used for boostrap, master and worker nodes.","title":"RHCOS and RHEL 8.X Images for OpenShift"},{"location":"ocp4-upi-kvm/docs/automation_host_prereqs/#download-the-rhel-qcow2-image","text":"Login to the RedHat portal and click on the Downloads tab. Click on the product Red Hat Enterprise Linux 8 from the list. In the \"Product Variant\" drop-down select \"Red Hat Enterprise Linux for Power, little endian\". In the \"Version\" drop-down select the version of RHEL you want to download. For example, will use the latest as 8.2. In the \"Product Software\" tab, click on \"Download Now\" button adjacent to \"Red Hat Enterprise Linux 8.2 Update KVM Guest Image\". Save the Qcow2 image. If your automation host is not Linux, then you will need to copy the Qcow2 image to a Linux host for the next step.","title":"Download the RHEL Qcow2 image"},{"location":"ocp4-upi-kvm/docs/automation_host_prereqs/#customize-the-rhel-qcow2-image","text":"Customize the Qcow2 image to set a root password and disable the cloud-init service. You will need the 'libguestfs-tools' package installed on the Linux machine to run the below command (Not available on Mac/Windows). # virt-customize -a <qcow2 image file name> --root-password password:<password> --uninstall cloud-init [ 0.0] Examining the guest ... [ 11.5] Setting a random seed [ 11.5] Uninstalling packages: cloud-init [ 13.9] Setting passwords [ 15.6] Finishing off On successful completion, copy the image back to the automation host (if it was not Linux) or make it available via an HTTP(S) server. Update the following Terraform input variables: * rhel_password = \"<password>\"","title":"Customize the RHEL Qcow2 image"},{"location":"ocp4-upi-kvm/docs/automation_host_prereqs/#download-the-rhcos-qcow2-image","text":"Select the RHCOS version you need from the OpenShift mirror . Find and download the QEMU qcow2 image gzip file. eg: rhcos-4.4.9-ppc64le-qemu.ppc64le.qcow2.gz Extract the Qcow2 image and place it on the automation host or make it available via an HTTP(S) server.","title":"Download the RHCOS Qcow2 image"},{"location":"ocp4-upi-kvm/docs/libvirt-host-setup/","text":"Libvirt Host Setup Libvirt Host Setup Libvirt Configuration Install and Configure Libvirt Enable IP Forwarding Accept TCP connections Allow Firewall for libvirt connections Password less access Verification Libvirt Configuration The following steps are required to configure Libvirt to work with the automation code Install and Configure Libvirt Please follow the steps given at Install and Enable section Enable IP Forwarding Please follow the steps given at IP forwarding section Accept TCP connections Please follow the steps given at Accept TCP connections section Allow Firewall for libvirt connections Please follow the steps given at Firewalld section Password less access Follow below steps to add the public key of the Terraform client machine user to the authorized list for password-less access. 1. Copy ~/.ssh/id_rsa.pub contents from Terraform client machine. 2. Append the public key as copied above to ~/.ssh/authorized_keys on the KVM host. Verification You can verify TCP connections to the kvm host using an example virsh command given below. virsh --connect qemu+tcp://<host_name_or_ip>/system --readonly","title":"Libvirt Host Setup"},{"location":"ocp4-upi-kvm/docs/libvirt-host-setup/#libvirt-host-setup","text":"Libvirt Host Setup Libvirt Configuration Install and Configure Libvirt Enable IP Forwarding Accept TCP connections Allow Firewall for libvirt connections Password less access Verification","title":"Libvirt Host Setup"},{"location":"ocp4-upi-kvm/docs/libvirt-host-setup/#libvirt-configuration","text":"The following steps are required to configure Libvirt to work with the automation code","title":"Libvirt Configuration"},{"location":"ocp4-upi-kvm/docs/libvirt-host-setup/#install-and-configure-libvirt","text":"Please follow the steps given at Install and Enable section","title":"Install and Configure Libvirt"},{"location":"ocp4-upi-kvm/docs/libvirt-host-setup/#enable-ip-forwarding","text":"Please follow the steps given at IP forwarding section","title":"Enable IP Forwarding"},{"location":"ocp4-upi-kvm/docs/libvirt-host-setup/#accept-tcp-connections","text":"Please follow the steps given at Accept TCP connections section","title":"Accept TCP connections"},{"location":"ocp4-upi-kvm/docs/libvirt-host-setup/#allow-firewall-for-libvirt-connections","text":"Please follow the steps given at Firewalld section","title":"Allow Firewall for libvirt connections"},{"location":"ocp4-upi-kvm/docs/libvirt-host-setup/#password-less-access","text":"Follow below steps to add the public key of the Terraform client machine user to the authorized list for password-less access. 1. Copy ~/.ssh/id_rsa.pub contents from Terraform client machine. 2. Append the public key as copied above to ~/.ssh/authorized_keys on the KVM host.","title":"Password less access"},{"location":"ocp4-upi-kvm/docs/libvirt-host-setup/#verification","text":"You can verify TCP connections to the kvm host using an example virsh command given below. virsh --connect qemu+tcp://<host_name_or_ip>/system --readonly","title":"Verification"},{"location":"ocp4-upi-kvm/docs/quickstart/","text":"Installation Quickstart Installation Quickstart Download the Automation Code Setup Terraform Variables Start Install Post Install Delete Bootstrap Node Create API and Ingress DNS Records Cluster Access Using CLI Using Web UI Clean up Download the Automation Code You'll need to use git to clone the deployment code when working off the master branch git clone https://github.com/ocp-power-automation/ocp4-upi-kvm.git cd ocp4_upi_kvm All further instructions assumes you are in the code directory eg. ocp4-upi-kvm Setup Terraform Variables Update the var.tfvars based on your environment. Description of the variables are available in the following link . You can use environment variables for sensitive data that should not be saved to disk. $ set +o history $ export RHEL_SUBS_USERNAME=xxxxxxxxxxxxxxx $ export RHEL_SUBS_PASSWORD=xxxxxxxxxxxxxxx $ set -o history Start Install Run the following commands from within the directory. $ terraform init $ terraform apply -var-file var.tfvars If using environment variables for sensitive data, then do the following, instead. $ terraform init $ terraform apply -var-file var.tfvars -var rhel_subscription_username=\"$RHEL_SUBS_USERNAME\" -var rhel_subscription_password=\"$RHEL_SUBS_PASSWORD\" Now wait for the installation to complete. It may take around 40 mins to complete provisioning. On successful install cluster details will be printed as shown below. bastion_ip = 192.168.61.2 bastion_ssh_command = ssh root@192.168.61.2 bootstrap_ip = 192.168.61.3 cluster_id = test-cluster-9a4f etc_hosts_entries = 192.168.61.2 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com install_status = COMPLETED master_ips = [ \"192.168.61.4\", \"192.168.61.5\", \"192.168.61.6\", ] oc_server_url = https://api.test-cluster-9a4f.mydomain.com:6443/ storageclass_name = nfs-storage-provisioner web_console_url = https://console-openshift-console.apps.test-cluster-9a4f.mydomain.com worker_ips = [] These details can be retrieved anytime by running the following command from the root folder of the code $ terraform output Post Install Delete Bootstrap Node Once the deployment is completed successfully, you can safely delete the bootstrap node. This step is optional but recommended so as to free up the resources used. Change the count value to 0 in bootstrap map variable and re-run the apply command. Eg: bootstrap = { memory = 8192, vcpu = 4, count = 0 } Run command terraform apply -var-file var.tfvars Create API and Ingress DNS Records You can use one of the following options. Add entries to your DNS server The general format is shown below: api.<cluster_id>. IN A <bastion_ip> *.apps.<cluster_id>. IN A <bastion_ip> You'll need bastion_ip and cluster_id . This is printed at the end of a successful install. Or you can retrieve it anytime by running terraform output from the install directory. For example, if bastion_ip = 192.168.61.2 and cluster_id = test-cluster-9a4f then the following DNS records will need to be added. api.test-cluster-9a4f. IN A 192.168.61.2 *.apps.test-cluster-9a4f. IN A 192.168.61.2 Add entries to your client system hosts file For Linux and Mac hosts file is located at /etc/hosts and for Windows it's located at c:\\Windows\\System32\\Drivers\\etc\\hosts . The general format is shown below: <bastion_ip> api.<cluster_id> <bastion_ip> console-openshift-console.apps.<cluster_id> <bastion_ip> integrated-oauth-server-openshift-authentication.apps.<cluster_id> <bastion_ip> oauth-openshift.apps.<cluster_id> <bastion_ip> prometheus-k8s-openshift-monitoring.apps.<cluster_id> <bastion_ip> grafana-openshift-monitoring.apps.<cluster_id> <bastion_ip> <app name>.apps.<cluster_id> You'll need etc_host_entries . This is printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. As an example, for the following etc_hosts_entries etc_hosts_entries = 192.168.61.2 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com just add the following entry to the hosts file ``` [existing entries in hosts file] 192.168.61.2 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com ``` Cluster Access OpenShift login credentials are in the bastion host and the location will be printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. [...] bastion_ip = 192.168.61.2 bastion_ssh_command = ssh root@192.168.61.2 [...] There are two files under ~/openstack-upi/auth - kubeconfig : can be used for CLI access - kubeadmin-password : Password for kubeadmin user which can be used for CLI, UI access Note : Ensure you securely store the OpenShift cluster access credentials. If desired delete the access details from the bastion node after securely storing the same. You can copy the access details to your local system $ scp -r -i data/id_rsa root@1192.168.61.2:~/openstack-upi/auth/\\* . Using CLI OpenShift CLI oc can be downloaded from the following links. Use the one specific to your client system architecture. Mac OSX Linux (x86_64) Linux (ppc64le) Windows Download the specific file, extract it and place the binary in a directory that is on your PATH For more details check the following link The CLI login URL oc_server_url will be printed at the end of successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. [...] oc_server_url = https://test-cluster-9a4f.mydomain.com:6443 [...] In order to login the cluster you can use the oc login <oc_server_url> -u kubeadmin -p <kubeadmin-password> Example: $ oc login https://test-cluster-9a4f.mydomain.com:6443 -u kubeadmin -p $(cat kubeadmin-password) You can also use the kubeconfig file $ export KUBECONFIG=$(pwd)/kubeconfig $ oc cluster-info Kubernetes master is running at https://test-cluster-9a4f.mydomain.com:6443 To further debug and diagnose cluster problems, use 'kubectl cluster-info dump' $ oc get nodes NAME STATUS ROLES AGE VERSION master-0 Ready master 13h v1.18.3+b74c5ed master-1 Ready master 13h v1.18.3+b74c5ed master-2 Ready master 13h v1.18.3+b74c5ed worker-0 Ready worker 13h v1.18.3+b74c5ed worker-1 Ready worker 13h v1.18.3+b74c5ed Note: The OpenShift command-line client oc is already configured on the bastion node with kubeconfig placed at ~/.kube/config . Using Web UI The web console URL will be printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. [...] web_console_url = https://console-openshift-console.apps.test-cluster-9a4f.mydomain.com [...] Open this URL in your browser and login with user kubeadmin and password mentioned in the kubeadmin-password file. Clean up To destroy after you are done using the cluster you can run command terraform destroy -var-file var.tfvars to make sure that all resources are properly cleaned up. Do not manually clean up your environment unless both of the following are true: You know what you are doing Something went wrong with an automated deletion.","title":"Installation Quickstart"},{"location":"ocp4-upi-kvm/docs/quickstart/#installation-quickstart","text":"Installation Quickstart Download the Automation Code Setup Terraform Variables Start Install Post Install Delete Bootstrap Node Create API and Ingress DNS Records Cluster Access Using CLI Using Web UI Clean up","title":"Installation Quickstart"},{"location":"ocp4-upi-kvm/docs/quickstart/#download-the-automation-code","text":"You'll need to use git to clone the deployment code when working off the master branch git clone https://github.com/ocp-power-automation/ocp4-upi-kvm.git cd ocp4_upi_kvm All further instructions assumes you are in the code directory eg. ocp4-upi-kvm","title":"Download the Automation Code"},{"location":"ocp4-upi-kvm/docs/quickstart/#setup-terraform-variables","text":"Update the var.tfvars based on your environment. Description of the variables are available in the following link . You can use environment variables for sensitive data that should not be saved to disk. $ set +o history $ export RHEL_SUBS_USERNAME=xxxxxxxxxxxxxxx $ export RHEL_SUBS_PASSWORD=xxxxxxxxxxxxxxx $ set -o history","title":"Setup Terraform Variables"},{"location":"ocp4-upi-kvm/docs/quickstart/#start-install","text":"Run the following commands from within the directory. $ terraform init $ terraform apply -var-file var.tfvars If using environment variables for sensitive data, then do the following, instead. $ terraform init $ terraform apply -var-file var.tfvars -var rhel_subscription_username=\"$RHEL_SUBS_USERNAME\" -var rhel_subscription_password=\"$RHEL_SUBS_PASSWORD\" Now wait for the installation to complete. It may take around 40 mins to complete provisioning. On successful install cluster details will be printed as shown below. bastion_ip = 192.168.61.2 bastion_ssh_command = ssh root@192.168.61.2 bootstrap_ip = 192.168.61.3 cluster_id = test-cluster-9a4f etc_hosts_entries = 192.168.61.2 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com install_status = COMPLETED master_ips = [ \"192.168.61.4\", \"192.168.61.5\", \"192.168.61.6\", ] oc_server_url = https://api.test-cluster-9a4f.mydomain.com:6443/ storageclass_name = nfs-storage-provisioner web_console_url = https://console-openshift-console.apps.test-cluster-9a4f.mydomain.com worker_ips = [] These details can be retrieved anytime by running the following command from the root folder of the code $ terraform output","title":"Start Install"},{"location":"ocp4-upi-kvm/docs/quickstart/#post-install","text":"","title":"Post Install"},{"location":"ocp4-upi-kvm/docs/quickstart/#delete-bootstrap-node","text":"Once the deployment is completed successfully, you can safely delete the bootstrap node. This step is optional but recommended so as to free up the resources used. Change the count value to 0 in bootstrap map variable and re-run the apply command. Eg: bootstrap = { memory = 8192, vcpu = 4, count = 0 } Run command terraform apply -var-file var.tfvars","title":"Delete Bootstrap Node"},{"location":"ocp4-upi-kvm/docs/quickstart/#create-api-and-ingress-dns-records","text":"You can use one of the following options. Add entries to your DNS server The general format is shown below: api.<cluster_id>. IN A <bastion_ip> *.apps.<cluster_id>. IN A <bastion_ip> You'll need bastion_ip and cluster_id . This is printed at the end of a successful install. Or you can retrieve it anytime by running terraform output from the install directory. For example, if bastion_ip = 192.168.61.2 and cluster_id = test-cluster-9a4f then the following DNS records will need to be added. api.test-cluster-9a4f. IN A 192.168.61.2 *.apps.test-cluster-9a4f. IN A 192.168.61.2 Add entries to your client system hosts file For Linux and Mac hosts file is located at /etc/hosts and for Windows it's located at c:\\Windows\\System32\\Drivers\\etc\\hosts . The general format is shown below: <bastion_ip> api.<cluster_id> <bastion_ip> console-openshift-console.apps.<cluster_id> <bastion_ip> integrated-oauth-server-openshift-authentication.apps.<cluster_id> <bastion_ip> oauth-openshift.apps.<cluster_id> <bastion_ip> prometheus-k8s-openshift-monitoring.apps.<cluster_id> <bastion_ip> grafana-openshift-monitoring.apps.<cluster_id> <bastion_ip> <app name>.apps.<cluster_id> You'll need etc_host_entries . This is printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. As an example, for the following etc_hosts_entries etc_hosts_entries = 192.168.61.2 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com just add the following entry to the hosts file ``` [existing entries in hosts file] 192.168.61.2 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com ```","title":"Create API and Ingress DNS Records"},{"location":"ocp4-upi-kvm/docs/quickstart/#cluster-access","text":"OpenShift login credentials are in the bastion host and the location will be printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. [...] bastion_ip = 192.168.61.2 bastion_ssh_command = ssh root@192.168.61.2 [...] There are two files under ~/openstack-upi/auth - kubeconfig : can be used for CLI access - kubeadmin-password : Password for kubeadmin user which can be used for CLI, UI access Note : Ensure you securely store the OpenShift cluster access credentials. If desired delete the access details from the bastion node after securely storing the same. You can copy the access details to your local system $ scp -r -i data/id_rsa root@1192.168.61.2:~/openstack-upi/auth/\\* .","title":"Cluster Access"},{"location":"ocp4-upi-kvm/docs/quickstart/#using-cli","text":"OpenShift CLI oc can be downloaded from the following links. Use the one specific to your client system architecture. Mac OSX Linux (x86_64) Linux (ppc64le) Windows Download the specific file, extract it and place the binary in a directory that is on your PATH For more details check the following link The CLI login URL oc_server_url will be printed at the end of successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. [...] oc_server_url = https://test-cluster-9a4f.mydomain.com:6443 [...] In order to login the cluster you can use the oc login <oc_server_url> -u kubeadmin -p <kubeadmin-password> Example: $ oc login https://test-cluster-9a4f.mydomain.com:6443 -u kubeadmin -p $(cat kubeadmin-password) You can also use the kubeconfig file $ export KUBECONFIG=$(pwd)/kubeconfig $ oc cluster-info Kubernetes master is running at https://test-cluster-9a4f.mydomain.com:6443 To further debug and diagnose cluster problems, use 'kubectl cluster-info dump' $ oc get nodes NAME STATUS ROLES AGE VERSION master-0 Ready master 13h v1.18.3+b74c5ed master-1 Ready master 13h v1.18.3+b74c5ed master-2 Ready master 13h v1.18.3+b74c5ed worker-0 Ready worker 13h v1.18.3+b74c5ed worker-1 Ready worker 13h v1.18.3+b74c5ed Note: The OpenShift command-line client oc is already configured on the bastion node with kubeconfig placed at ~/.kube/config .","title":"Using CLI"},{"location":"ocp4-upi-kvm/docs/quickstart/#using-web-ui","text":"The web console URL will be printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. [...] web_console_url = https://console-openshift-console.apps.test-cluster-9a4f.mydomain.com [...] Open this URL in your browser and login with user kubeadmin and password mentioned in the kubeadmin-password file.","title":"Using Web UI"},{"location":"ocp4-upi-kvm/docs/quickstart/#clean-up","text":"To destroy after you are done using the cluster you can run command terraform destroy -var-file var.tfvars to make sure that all resources are properly cleaned up. Do not manually clean up your environment unless both of the following are true: You know what you are doing Something went wrong with an automated deletion.","title":"Clean up"},{"location":"ocp4-upi-kvm/docs/terraform-provider-build/","text":"Terraform Providers At present Terraform registry does not support some plugins. Third-party providers can be manually installed using local filesystem as a mirror . This is in addition to the provider plugins that are downloaded by Terraform during terraform init . Follow below steps based on your Terraform client machine to setup terraform providers. These steps are required to be followed before running the automation. Most of the steps require Go to be installed from https://golang.org/dl/. We recommed Go version above 1.14. Make sure to set your GOPATH environment variable and add $GOPATH/bin to PATH. On Mac/Linux Identify your platform. All example commands assume Linux as a platform: Linux: linux_amd64 Mac OSX: darwin_amd64 Identify your Terraform plugin directory. You will need to create the directory on your client machine: Linux: ~/.local/share/terraform/plugins OR /usr/local/share/terraform/plugins OR /usr/share/terraform/plugins. Mac OSX: ~/Library/Application Support/io.terraform/plugins OR /Library/Application Support/io.terraform/plugins Libvirt provider : Please refer to the section below for instructions on installing the libvirt provider plugin. For more information . Run below commands to install libvirt provider. Make sure to change PLATFORM and PLUGIN_PATH values based on your client machine. PLATFORM=linux_amd64 PLUGIN_PATH=~/.local/share/terraform/plugins #Install the Libvirt provider plugin: git clone https://github.com/dmacvicar/terraform-provider-libvirt $GOPATH/src/github.com/dmacvicar/terraform-provider-libvirt cd $GOPATH/src/github.com/dmacvicar/terraform-provider-libvirt make install cd - # Create plugin directory and copy the binary mkdir -p $PLUGIN_PATH/registry.terraform.io/dmacvicar/libvirt/1.0.0/$PLATFORM/ cp $GOPATH/bin/terraform-provider-libvirt $PLUGIN_PATH/registry.terraform.io/dmacvicar/libvirt/1.0.0/$PLATFORM/terraform-provider-libvirt Upon successful completion. Please follow the instructions outlined here https://github.com/ocp-power-automation/ocp4-upi-kvm/blob/master/docs/quickstart.md#start-install On IBM Power Systems Identify your platform. All example commands assume Linux as a platform: Linux: linux_ppc64le Identify your Terraform plugin directory. You will need to create the directory on your client machine: Linux: ~/.local/share/terraform/plugins OR /usr/local/share/terraform/plugins OR /usr/share/terraform/plugins. Make sure to change PLATFORM and PLUGIN_PATH values based on your client machine. terraform-provider-libvirt : Please refer to the section below for instructions on installing the libvirt provider plugin. For more information . PLATFORM=linux_ppc64le PLUGIN_PATH=~/.local/share/terraform/plugins #Install the Libvirt provider plugin: git clone https://github.com/dmacvicar/terraform-provider-libvirt $GOPATH/src/github.com/dmacvicar/terraform-provider-libvirt cd $GOPATH/src/github.com/dmacvicar/terraform-provider-libvirt make install cd - # Create plugin directory and copy the binary mkdir -p $PLUGIN_PATH/registry.terraform.io/dmacvicar/libvirt/1.0.0/$PLATFORM/ cp $GOPATH/bin/terraform-provider-libvirt $PLUGIN_PATH/registry.terraform.io/dmacvicar/libvirt/1.0.0/$PLATFORM/terraform-provider-libvirt terraform-provider-random : Please refer to the section below for instructions on installing the random provider plugin. For more information . Note: Set VERSION to a compatible version eg. 2.3.0 VERSION=2.3.0 PLATFORM=linux_ppc64le PLUGIN_PATH=~/.local/share/terraform/plugins #Install the random provider plugin: git clone https://github.com/hashicorp/terraform-provider-random --branch v$VERSION $GOPATH/src/github.com/hashicorp/terraform-provider-random cd $GOPATH/src/github.com/hashicorp/terraform-provider-random make build cd - # Create plugin directory and copy the binary mkdir -p $PLUGIN_PATH/registry.terraform.io/hashicorp/random/$VERSION/$PLATFORM/ cp $GOPATH/bin/terraform-provider-random $PLUGIN_PATH/registry.terraform.io/hashicorp/random/$VERSION/$PLATFORM/terraform-provider-random terraform-provider-ignition : Please refer to the section below for instructions on installing the ignition provider plugin. For more information . Note: Set VERSION to a compatible version eg. 2.1.0 VERSION=2.1.0 PLATFORM=linux_ppc64le PLUGIN_PATH=~/.local/share/terraform/plugins #Install the ignition provider plugin: git clone https://github.com/community-terraform-providers/terraform-provider-ignition --branch v$VERSION $GOPATH/src/github.com/terraform-providers/terraform-provider-ignition cd $GOPATH/src/github.com/terraform-providers/terraform-provider-ignition make build cd - # Create plugin directory and copy the binary mkdir -p $PLUGIN_PATH/registry.terraform.io/terraform-providers/ignition/$VERSION/$PLATFORM/ cp $GOPATH/bin/terraform-provider-ignition $PLUGIN_PATH/registry.terraform.io/terraform-providers/ignition/$VERSION/$PLATFORM/terraform-provider-ignition terraform-provider-null : Please refer to the section below for instructions on installing the null provider plugin. For more information . Note: Set VERSION to a compatible version eg. 2.1.2 VERSION=2.1.2 PLATFORM=linux_ppc64le PLUGIN_PATH=~/.local/share/terraform/plugins #Install the null provider plugin: git clone https://github.com/hashicorp/terraform-provider-null --branch v$VERSION $GOPATH/src/github.com/hashicorp/terraform-provider-null cd $GOPATH/src/github.com/hashicorp/terraform-provider-null make build cd - # Create plugin directory and copy the binary mkdir -p $PLUGIN_PATH/registry.terraform.io/hashicorp/null/$VERSION/$PLATFORM/ cp $GOPATH/bin/terraform-provider-null $PLUGIN_PATH/registry.terraform.io/hashicorp/null/$VERSION/$PLATFORM/terraform-provider-null Upon successful completion. Please follow the instructions outlined here https://github.com/ocp-power-automation/ocp4-upi-kvm/blob/master/docs/quickstart.md#start-install","title":"Terraform Providers"},{"location":"ocp4-upi-kvm/docs/terraform-provider-build/#terraform-providers","text":"At present Terraform registry does not support some plugins. Third-party providers can be manually installed using local filesystem as a mirror . This is in addition to the provider plugins that are downloaded by Terraform during terraform init . Follow below steps based on your Terraform client machine to setup terraform providers. These steps are required to be followed before running the automation. Most of the steps require Go to be installed from https://golang.org/dl/. We recommed Go version above 1.14. Make sure to set your GOPATH environment variable and add $GOPATH/bin to PATH.","title":"Terraform Providers"},{"location":"ocp4-upi-kvm/docs/terraform-provider-build/#on-maclinux","text":"Identify your platform. All example commands assume Linux as a platform: Linux: linux_amd64 Mac OSX: darwin_amd64 Identify your Terraform plugin directory. You will need to create the directory on your client machine: Linux: ~/.local/share/terraform/plugins OR /usr/local/share/terraform/plugins OR /usr/share/terraform/plugins. Mac OSX: ~/Library/Application Support/io.terraform/plugins OR /Library/Application Support/io.terraform/plugins Libvirt provider : Please refer to the section below for instructions on installing the libvirt provider plugin. For more information . Run below commands to install libvirt provider. Make sure to change PLATFORM and PLUGIN_PATH values based on your client machine. PLATFORM=linux_amd64 PLUGIN_PATH=~/.local/share/terraform/plugins #Install the Libvirt provider plugin: git clone https://github.com/dmacvicar/terraform-provider-libvirt $GOPATH/src/github.com/dmacvicar/terraform-provider-libvirt cd $GOPATH/src/github.com/dmacvicar/terraform-provider-libvirt make install cd - # Create plugin directory and copy the binary mkdir -p $PLUGIN_PATH/registry.terraform.io/dmacvicar/libvirt/1.0.0/$PLATFORM/ cp $GOPATH/bin/terraform-provider-libvirt $PLUGIN_PATH/registry.terraform.io/dmacvicar/libvirt/1.0.0/$PLATFORM/terraform-provider-libvirt Upon successful completion. Please follow the instructions outlined here https://github.com/ocp-power-automation/ocp4-upi-kvm/blob/master/docs/quickstart.md#start-install","title":"On Mac/Linux"},{"location":"ocp4-upi-kvm/docs/terraform-provider-build/#on-ibm-power-systems","text":"Identify your platform. All example commands assume Linux as a platform: Linux: linux_ppc64le Identify your Terraform plugin directory. You will need to create the directory on your client machine: Linux: ~/.local/share/terraform/plugins OR /usr/local/share/terraform/plugins OR /usr/share/terraform/plugins. Make sure to change PLATFORM and PLUGIN_PATH values based on your client machine. terraform-provider-libvirt : Please refer to the section below for instructions on installing the libvirt provider plugin. For more information . PLATFORM=linux_ppc64le PLUGIN_PATH=~/.local/share/terraform/plugins #Install the Libvirt provider plugin: git clone https://github.com/dmacvicar/terraform-provider-libvirt $GOPATH/src/github.com/dmacvicar/terraform-provider-libvirt cd $GOPATH/src/github.com/dmacvicar/terraform-provider-libvirt make install cd - # Create plugin directory and copy the binary mkdir -p $PLUGIN_PATH/registry.terraform.io/dmacvicar/libvirt/1.0.0/$PLATFORM/ cp $GOPATH/bin/terraform-provider-libvirt $PLUGIN_PATH/registry.terraform.io/dmacvicar/libvirt/1.0.0/$PLATFORM/terraform-provider-libvirt terraform-provider-random : Please refer to the section below for instructions on installing the random provider plugin. For more information . Note: Set VERSION to a compatible version eg. 2.3.0 VERSION=2.3.0 PLATFORM=linux_ppc64le PLUGIN_PATH=~/.local/share/terraform/plugins #Install the random provider plugin: git clone https://github.com/hashicorp/terraform-provider-random --branch v$VERSION $GOPATH/src/github.com/hashicorp/terraform-provider-random cd $GOPATH/src/github.com/hashicorp/terraform-provider-random make build cd - # Create plugin directory and copy the binary mkdir -p $PLUGIN_PATH/registry.terraform.io/hashicorp/random/$VERSION/$PLATFORM/ cp $GOPATH/bin/terraform-provider-random $PLUGIN_PATH/registry.terraform.io/hashicorp/random/$VERSION/$PLATFORM/terraform-provider-random terraform-provider-ignition : Please refer to the section below for instructions on installing the ignition provider plugin. For more information . Note: Set VERSION to a compatible version eg. 2.1.0 VERSION=2.1.0 PLATFORM=linux_ppc64le PLUGIN_PATH=~/.local/share/terraform/plugins #Install the ignition provider plugin: git clone https://github.com/community-terraform-providers/terraform-provider-ignition --branch v$VERSION $GOPATH/src/github.com/terraform-providers/terraform-provider-ignition cd $GOPATH/src/github.com/terraform-providers/terraform-provider-ignition make build cd - # Create plugin directory and copy the binary mkdir -p $PLUGIN_PATH/registry.terraform.io/terraform-providers/ignition/$VERSION/$PLATFORM/ cp $GOPATH/bin/terraform-provider-ignition $PLUGIN_PATH/registry.terraform.io/terraform-providers/ignition/$VERSION/$PLATFORM/terraform-provider-ignition terraform-provider-null : Please refer to the section below for instructions on installing the null provider plugin. For more information . Note: Set VERSION to a compatible version eg. 2.1.2 VERSION=2.1.2 PLATFORM=linux_ppc64le PLUGIN_PATH=~/.local/share/terraform/plugins #Install the null provider plugin: git clone https://github.com/hashicorp/terraform-provider-null --branch v$VERSION $GOPATH/src/github.com/hashicorp/terraform-provider-null cd $GOPATH/src/github.com/hashicorp/terraform-provider-null make build cd - # Create plugin directory and copy the binary mkdir -p $PLUGIN_PATH/registry.terraform.io/hashicorp/null/$VERSION/$PLATFORM/ cp $GOPATH/bin/terraform-provider-null $PLUGIN_PATH/registry.terraform.io/hashicorp/null/$VERSION/$PLATFORM/terraform-provider-null Upon successful completion. Please follow the instructions outlined here https://github.com/ocp-power-automation/ocp4-upi-kvm/blob/master/docs/quickstart.md#start-install","title":"On IBM Power Systems"},{"location":"ocp4-upi-kvm/docs/var.tfvars-doc/","text":"How to use var.tfvars How to use var.tfvars Introduction Libvirt Details OpenShift Cluster Details OpenShift Installation Details Misc Customizations Introduction This guide gives an overview of the various terraform variables that are used for the deployment. The default values are set in variables.tf Libvirt Details These set of variables specify the Libvirt details. libvirt_uri = \"qemu+tcp://localhost/system\" host_address = \"\" images_path = \"/home/libvirt/openshift-images\" OpenShift Cluster Details These set of variables specify the cluster capacity. bastion = { memory = 8192, vcpu = 2 } bootstrap = { memory = 8192, vcpu = 4, count = 1 } master = { memory = 16384, vcpu = 4, count = 3 } worker = { memory = 16384, vcpu = 4, count = 2 } You can optionally set worker count value to 0 in which case all the cluster pods will be running on the master/supervisor nodes. Ensure you use proper sizing for master/supervisor nodes to avoid resource starvation for containers. These set of variables specify the RHEL and RHCOS boot image location. Ensure that you use the correct RHCOS image specific to the pre-release version. bastion_image = \"<url-or-path-to-rhel-qcow2>\" rhcos_image = \"<url-or-path-to-rhcos-qcow2>\" These set of variables specify the username, password and the SSH key to be used for accessing the bastion node. rhel_username = \"root\" rhel_password = \"<password>\" public_key_file = \"data/id_rsa.pub\" private_key_file = \"data/id_rsa\" Please note that only OpenSSH formatted keys are supported. Refer to the following links for instructions on creating SSH key based on your platform. - Windows 10 - https://phoenixnap.com/kb/generate-ssh-key-windows-10 - Mac OSX - https://www.techrepublic.com/article/how-to-generate-ssh-keys-on-macos-mojave/ - Linux - https://www.siteground.com/kb/generate_ssh_key_in_linux/ Create the SSH key-pair and keep it under the data directory These set of variables specify the RHEL subscription details. This is sensitive data, and if you don't want to save it on disk, use environment variables RHEL_SUBS_USERNAME and RHEL_SUBS_PASSWORD and pass them to terraform apply command as shown in the Quickstart guide . rhel_subscription_username = \"user@test.com\" rhel_subscription_password = \"mypassword\" If you have an org wide activation key, then use the following variables rhel_subscription_org = \"\" rhel_subscription_activationkey = \"\" OpenShift Installation Details These variables specify the URL for the OpenShift installer and client binaries. Change the URL to the specific pre-release version that you want to install on PowerVS. Reference link - https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview openshift_install_tarball = \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview/latest/openshift-install-linux.tar.gz\" openshift_client_tarball = \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview/latest/openshift-client-linux.tar.gz\" This variable specifies the OpenShift pull secret. This is available from the following link - https://cloud.redhat.com/openshift/install/power/user-provisioned Download the secret and copy it to data/pull-secret.txt . pull_secret_file = \"data/pull-secret.txt\" These variables specifies the OpenShift cluster domain details. Edit it as per your requirements. cluster_domain = \"ibm.com\" cluster_id_prefix = \"test-ocp\" cluster_id = \"\" The cluster_id_prefix should not be more than 8 characters. Nodes are pre-fixed with this value. Default value is test-ocp A random value will be used for cluster_id if not set. The total length of cluster_id_prefix . cluster_id should not exceed 14 characters. Misc Customizations These variables provides miscellaneous customizations. For common usage scenarios these are not required and should be left unchanged. The following variable is used to specify the Libvirt VM CPU mode. For example custom, host-passthrough, host-model. cpu_mode = \"\" The following variable is used to define the network subnet for the OCP cluster. Default is set to '192.168.27.0/24'. network_cidr = \"\" The following variables can be used for disconnected install by using a local mirror registry on the bastion node. enable_local_registry = false #Set to true to enable usage of local registry for restricted network install. local_registry_image = \"docker.io/ibmcom/registry-ppc64le:2.6.2.5\" ocp_release_tag = \"4.4.9-ppc64le\" This variable can be used for trying out custom OpenShift install image for development use. release_image_override = \"\" These variables specify the ansible playbooks that are used for OpenShift install and post-install customizations. helpernode_tag = \"5eab3db53976bb16be582f2edc2de02f7510050d\" install_playbook_tag = \"02a598faa332aa2c3d53e8edd0e840440ff74bd5\" GThese variables can be used when debugging ansible playbooks installer_log_level = \"info\" ansible_extra_options = \"-v\" This variable specifies the external DNS servers to forward DNS queries that cannot be resolved locally. dns_forwarders = \"1.1.1.1; 9.9.9.9\" These are NTP specific variables that are used for time-synchronization in the OpenShift cluster. chrony_config = true chrony_config_servers = [ {server = \"0.centos.pool.ntp.org\", options = \"iburst\"}, {server = \"1.centos.pool.ntp.org\", options = \"iburst\"} ] These variables specify details about NFS storage that is setup by default on the bastion server. storage_type = \"nfs\" volume_size = \"300\" # Value in GB The following variables are specific to upgrading an existing installation. upgrade_version = \"\" upgrade_channel = \"\" #(stable-4.x, fast-4.x, candidate-4.x) eg. stable-4.5 upgrade_pause_time = \"90\" upgrade_delay_time = \"600\"","title":"How to use var.tfvars"},{"location":"ocp4-upi-kvm/docs/var.tfvars-doc/#how-to-use-vartfvars","text":"How to use var.tfvars Introduction Libvirt Details OpenShift Cluster Details OpenShift Installation Details Misc Customizations","title":"How to use var.tfvars"},{"location":"ocp4-upi-kvm/docs/var.tfvars-doc/#introduction","text":"This guide gives an overview of the various terraform variables that are used for the deployment. The default values are set in variables.tf","title":"Introduction"},{"location":"ocp4-upi-kvm/docs/var.tfvars-doc/#libvirt-details","text":"These set of variables specify the Libvirt details. libvirt_uri = \"qemu+tcp://localhost/system\" host_address = \"\" images_path = \"/home/libvirt/openshift-images\"","title":"Libvirt Details"},{"location":"ocp4-upi-kvm/docs/var.tfvars-doc/#openshift-cluster-details","text":"These set of variables specify the cluster capacity. bastion = { memory = 8192, vcpu = 2 } bootstrap = { memory = 8192, vcpu = 4, count = 1 } master = { memory = 16384, vcpu = 4, count = 3 } worker = { memory = 16384, vcpu = 4, count = 2 } You can optionally set worker count value to 0 in which case all the cluster pods will be running on the master/supervisor nodes. Ensure you use proper sizing for master/supervisor nodes to avoid resource starvation for containers. These set of variables specify the RHEL and RHCOS boot image location. Ensure that you use the correct RHCOS image specific to the pre-release version. bastion_image = \"<url-or-path-to-rhel-qcow2>\" rhcos_image = \"<url-or-path-to-rhcos-qcow2>\" These set of variables specify the username, password and the SSH key to be used for accessing the bastion node. rhel_username = \"root\" rhel_password = \"<password>\" public_key_file = \"data/id_rsa.pub\" private_key_file = \"data/id_rsa\" Please note that only OpenSSH formatted keys are supported. Refer to the following links for instructions on creating SSH key based on your platform. - Windows 10 - https://phoenixnap.com/kb/generate-ssh-key-windows-10 - Mac OSX - https://www.techrepublic.com/article/how-to-generate-ssh-keys-on-macos-mojave/ - Linux - https://www.siteground.com/kb/generate_ssh_key_in_linux/ Create the SSH key-pair and keep it under the data directory These set of variables specify the RHEL subscription details. This is sensitive data, and if you don't want to save it on disk, use environment variables RHEL_SUBS_USERNAME and RHEL_SUBS_PASSWORD and pass them to terraform apply command as shown in the Quickstart guide . rhel_subscription_username = \"user@test.com\" rhel_subscription_password = \"mypassword\" If you have an org wide activation key, then use the following variables rhel_subscription_org = \"\" rhel_subscription_activationkey = \"\"","title":"OpenShift Cluster Details"},{"location":"ocp4-upi-kvm/docs/var.tfvars-doc/#openshift-installation-details","text":"These variables specify the URL for the OpenShift installer and client binaries. Change the URL to the specific pre-release version that you want to install on PowerVS. Reference link - https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview openshift_install_tarball = \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview/latest/openshift-install-linux.tar.gz\" openshift_client_tarball = \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview/latest/openshift-client-linux.tar.gz\" This variable specifies the OpenShift pull secret. This is available from the following link - https://cloud.redhat.com/openshift/install/power/user-provisioned Download the secret and copy it to data/pull-secret.txt . pull_secret_file = \"data/pull-secret.txt\" These variables specifies the OpenShift cluster domain details. Edit it as per your requirements. cluster_domain = \"ibm.com\" cluster_id_prefix = \"test-ocp\" cluster_id = \"\" The cluster_id_prefix should not be more than 8 characters. Nodes are pre-fixed with this value. Default value is test-ocp A random value will be used for cluster_id if not set. The total length of cluster_id_prefix . cluster_id should not exceed 14 characters.","title":"OpenShift Installation Details"},{"location":"ocp4-upi-kvm/docs/var.tfvars-doc/#misc-customizations","text":"These variables provides miscellaneous customizations. For common usage scenarios these are not required and should be left unchanged. The following variable is used to specify the Libvirt VM CPU mode. For example custom, host-passthrough, host-model. cpu_mode = \"\" The following variable is used to define the network subnet for the OCP cluster. Default is set to '192.168.27.0/24'. network_cidr = \"\" The following variables can be used for disconnected install by using a local mirror registry on the bastion node. enable_local_registry = false #Set to true to enable usage of local registry for restricted network install. local_registry_image = \"docker.io/ibmcom/registry-ppc64le:2.6.2.5\" ocp_release_tag = \"4.4.9-ppc64le\" This variable can be used for trying out custom OpenShift install image for development use. release_image_override = \"\" These variables specify the ansible playbooks that are used for OpenShift install and post-install customizations. helpernode_tag = \"5eab3db53976bb16be582f2edc2de02f7510050d\" install_playbook_tag = \"02a598faa332aa2c3d53e8edd0e840440ff74bd5\" GThese variables can be used when debugging ansible playbooks installer_log_level = \"info\" ansible_extra_options = \"-v\" This variable specifies the external DNS servers to forward DNS queries that cannot be resolved locally. dns_forwarders = \"1.1.1.1; 9.9.9.9\" These are NTP specific variables that are used for time-synchronization in the OpenShift cluster. chrony_config = true chrony_config_servers = [ {server = \"0.centos.pool.ntp.org\", options = \"iburst\"}, {server = \"1.centos.pool.ntp.org\", options = \"iburst\"} ] These variables specify details about NFS storage that is setup by default on the bastion server. storage_type = \"nfs\" volume_size = \"300\" # Value in GB The following variables are specific to upgrading an existing installation. upgrade_version = \"\" upgrade_channel = \"\" #(stable-4.x, fast-4.x, candidate-4.x) eg. stable-4.5 upgrade_pause_time = \"90\" upgrade_delay_time = \"600\"","title":"Misc Customizations"},{"location":"ocp4-upi-powervm/","text":"Table of Contents Table of Contents Introduction Automation Host Prerequisites PowerVC Prerequisites OCP Install Contributing Introduction The ocp4-upi-powervm project provides Terraform based automation code to help the deployment of OpenShift Container Platform (OCP) 4.x on PowerVM systems managed by PowerVC. If you are using standalone PowerVM please take a look at the following quickstart guide which uses the ansible playbook to setup a helper node (bastion) for OCP deployment. This project also leverages the same ansible playbook internally for OCP deployment on PowerVM LPARs managed via PowerVC. :heavy_exclamation_mark: For bugs/enhancement requests etc. please open a GitHub issue :information_source: The main branch must be used with latest OCP pre-release versions only. For stable releases please checkout specific release branches - { release-4.5 , release-4.6 ...} and follow the docs in the specific release branches. Automation Host Prerequisites The automation needs to run from a system with internet access. This could be your laptop or a VM with public internet connectivity. This automation code has been tested on the following 64-bit Operating Systems: - Mac OSX (Darwin) - Linux (x86_64) Follow the guide to complete the prerequisites. PowerVC Prerequisites Follow the guide to complete the PowerVC prerequisites. OCP Install Follow the quickstart guide for OCP installation on PowerVM LPARs managed via PowerVC Contributing Please see the contributing doc for more details. PRs are most welcome !!","title":"Deploying OpenShift on PowerVM managed via PowerVC"},{"location":"ocp4-upi-powervm/#table-of-contents","text":"Table of Contents Introduction Automation Host Prerequisites PowerVC Prerequisites OCP Install Contributing","title":"Table of Contents"},{"location":"ocp4-upi-powervm/#introduction","text":"The ocp4-upi-powervm project provides Terraform based automation code to help the deployment of OpenShift Container Platform (OCP) 4.x on PowerVM systems managed by PowerVC. If you are using standalone PowerVM please take a look at the following quickstart guide which uses the ansible playbook to setup a helper node (bastion) for OCP deployment. This project also leverages the same ansible playbook internally for OCP deployment on PowerVM LPARs managed via PowerVC. :heavy_exclamation_mark: For bugs/enhancement requests etc. please open a GitHub issue :information_source: The main branch must be used with latest OCP pre-release versions only. For stable releases please checkout specific release branches - { release-4.5 , release-4.6 ...} and follow the docs in the specific release branches.","title":"Introduction"},{"location":"ocp4-upi-powervm/#automation-host-prerequisites","text":"The automation needs to run from a system with internet access. This could be your laptop or a VM with public internet connectivity. This automation code has been tested on the following 64-bit Operating Systems: - Mac OSX (Darwin) - Linux (x86_64) Follow the guide to complete the prerequisites.","title":"Automation Host Prerequisites"},{"location":"ocp4-upi-powervm/#powervc-prerequisites","text":"Follow the guide to complete the PowerVC prerequisites.","title":"PowerVC Prerequisites"},{"location":"ocp4-upi-powervm/#ocp-install","text":"Follow the quickstart guide for OCP installation on PowerVM LPARs managed via PowerVC","title":"OCP Install"},{"location":"ocp4-upi-powervm/#contributing","text":"Please see the contributing doc for more details. PRs are most welcome !!","title":"Contributing"},{"location":"ocp4-upi-powervm/CODE_OF_CONDUCT/","text":"Contributor Covenant Code of Conduct Our Pledge We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. Our Standards Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Enforcement Responsibilities Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate. Scope This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement. Contact with repository owners. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident. Enforcement Guidelines Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct: 1. Correction Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested. 2. Warning Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban. 3. Temporary Ban Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban. 4. Permanent Ban Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html. Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder .","title":"Contributor Covenant Code of Conduct"},{"location":"ocp4-upi-powervm/CODE_OF_CONDUCT/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"ocp4-upi-powervm/CODE_OF_CONDUCT/#our-pledge","text":"We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.","title":"Our Pledge"},{"location":"ocp4-upi-powervm/CODE_OF_CONDUCT/#our-standards","text":"Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"ocp4-upi-powervm/CODE_OF_CONDUCT/#enforcement-responsibilities","text":"Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.","title":"Enforcement Responsibilities"},{"location":"ocp4-upi-powervm/CODE_OF_CONDUCT/#scope","text":"This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.","title":"Scope"},{"location":"ocp4-upi-powervm/CODE_OF_CONDUCT/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement. Contact with repository owners. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident.","title":"Enforcement"},{"location":"ocp4-upi-powervm/CODE_OF_CONDUCT/#enforcement-guidelines","text":"Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:","title":"Enforcement Guidelines"},{"location":"ocp4-upi-powervm/CODE_OF_CONDUCT/#1-correction","text":"Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.","title":"1. Correction"},{"location":"ocp4-upi-powervm/CODE_OF_CONDUCT/#2-warning","text":"Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.","title":"2. Warning"},{"location":"ocp4-upi-powervm/CODE_OF_CONDUCT/#3-temporary-ban","text":"Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.","title":"3. Temporary Ban"},{"location":"ocp4-upi-powervm/CODE_OF_CONDUCT/#4-permanent-ban","text":"Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community.","title":"4. Permanent Ban"},{"location":"ocp4-upi-powervm/CODE_OF_CONDUCT/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html. Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder .","title":"Attribution"},{"location":"ocp4-upi-powervm/CONTRIBUTING/","text":"Contributing This project is Apache 2.0 Licenced and welcomes external contributions. When contributing to this repository, please first discuss the change you wish to make via an issue . Please note we have a code of conduct , please follow it in all your interactions with the project. Issues If you find any issue with the code or documentation please submit an issue . It is best to check out existing issues first to see if a similar one is open or has already been discussed. Pull Request Process To contribute code or documentation, please submit a pull request . Always take the latest update from upstream/master before creating a pull request. Ensure your changes work fine and have no syntax problems. Also, verify that it does not break the existing code flow. Update the README.md or relevant documents with details of changes to the code. This includes variables change, added or updated feature, change in steps, dependencies change, etc. Make use of proper commit message. Mention the issue# which you are planning to address eg: Fixes #38. After creating the pull request ensure you implement all the review comments given if any. Pull request will be merged only when it has at least two approvals from the list of reviewers. Please read Developer Certificate of Origin and sign-off your commit using command git commit -s . Spec Formatting Conventions Documents in this repository will adhere to the following rules: Lines are wrapped at 80 columns (when possible) Use spaces to indent your code. Do not use tab character, instead can use 2/4 spaces.","title":"Contributing"},{"location":"ocp4-upi-powervm/CONTRIBUTING/#contributing","text":"This project is Apache 2.0 Licenced and welcomes external contributions. When contributing to this repository, please first discuss the change you wish to make via an issue . Please note we have a code of conduct , please follow it in all your interactions with the project.","title":"Contributing"},{"location":"ocp4-upi-powervm/CONTRIBUTING/#issues","text":"If you find any issue with the code or documentation please submit an issue . It is best to check out existing issues first to see if a similar one is open or has already been discussed.","title":"Issues"},{"location":"ocp4-upi-powervm/CONTRIBUTING/#pull-request-process","text":"To contribute code or documentation, please submit a pull request . Always take the latest update from upstream/master before creating a pull request. Ensure your changes work fine and have no syntax problems. Also, verify that it does not break the existing code flow. Update the README.md or relevant documents with details of changes to the code. This includes variables change, added or updated feature, change in steps, dependencies change, etc. Make use of proper commit message. Mention the issue# which you are planning to address eg: Fixes #38. After creating the pull request ensure you implement all the review comments given if any. Pull request will be merged only when it has at least two approvals from the list of reviewers. Please read Developer Certificate of Origin and sign-off your commit using command git commit -s .","title":"Pull Request Process"},{"location":"ocp4-upi-powervm/CONTRIBUTING/#spec-formatting-conventions","text":"Documents in this repository will adhere to the following rules: Lines are wrapped at 80 columns (when possible) Use spaces to indent your code. Do not use tab character, instead can use 2/4 spaces.","title":"Spec Formatting Conventions"},{"location":"ocp4-upi-powervm/docs/automation_host_prereqs/","text":"Automation Host Prerequisites Automation Host Prerequisites Configure Your Firewall Automation Host Setup Terraform Git Configure Your Firewall If your system is behind a firewall, you will need to ensure the following ports are open in order to use ssh, http, and https: - 22, 443, 80 These additional ports are required for the ocp cli ( oc ) post-install: - 6443 Automation Host Setup Install the following packages on the automation host. Select the appropriate install binaries based on your automation host platform - Mac/Linux. Terraform Terraform >= 0.13.0 : Please refer to the link for instructions on installing Terraform. For validating the version run terraform version command after install. Git Git : Please refer to the link for instructions on installing Git.","title":"Automation Host Prerequisites"},{"location":"ocp4-upi-powervm/docs/automation_host_prereqs/#automation-host-prerequisites","text":"Automation Host Prerequisites Configure Your Firewall Automation Host Setup Terraform Git","title":"Automation Host Prerequisites"},{"location":"ocp4-upi-powervm/docs/automation_host_prereqs/#configure-your-firewall","text":"If your system is behind a firewall, you will need to ensure the following ports are open in order to use ssh, http, and https: - 22, 443, 80 These additional ports are required for the ocp cli ( oc ) post-install: - 6443","title":"Configure Your Firewall"},{"location":"ocp4-upi-powervm/docs/automation_host_prereqs/#automation-host-setup","text":"Install the following packages on the automation host. Select the appropriate install binaries based on your automation host platform - Mac/Linux.","title":"Automation Host Setup"},{"location":"ocp4-upi-powervm/docs/automation_host_prereqs/#terraform","text":"Terraform >= 0.13.0 : Please refer to the link for instructions on installing Terraform. For validating the version run terraform version command after install.","title":"Terraform"},{"location":"ocp4-upi-powervm/docs/automation_host_prereqs/#git","text":"Git : Please refer to the link for instructions on installing Git.","title":"Git"},{"location":"ocp4-upi-powervm/docs/ocp4-manual-deployment-dhcp/","text":"Create Master/Worker/Bootstrap Nodes Create the below PowerVM LPARS with empty disk volume attached (refer the documentation link for resource requirements) and note the MAC ID for each of the LPARs. bootstrap - 1 master - 3 worker - 2 Create and Setup Bastion Node Create RHEL 8.1 LPAR Login to the RHEL 8.1 LPAR and clone the OCP4 helpernode repo Use the following vars.yaml as a template and change the IP, network and related details according to your environment. --- disk: sda helper: name: \"helper\" ipaddr: \"192.168.7.77\" dns: domain: \"example.com\" clusterid: \"ocp4\" forwarder1: \"8.8.8.8\" forwarder2: \"8.8.4.4\" dhcp: router: \"192.168.7.1\" bcast: \"192.168.7.255\" netmask: \"255.255.255.0\" poolstart: \"192.168.7.10\" poolend: \"192.168.7.30\" ipid: \"192.168.7.0\" netmaskid: \"255.255.255.0\" bootstrap: name: \"bootstrap\" ipaddr: \"192.168.7.20\" macaddr: \"52:54:00:60:72:67\" masters: - name: \"master0\" ipaddr: \"192.168.7.21\" macaddr: \"52:54:00:e7:9d:67\" - name: \"master1\" ipaddr: \"192.168.7.22\" macaddr: \"52:54:00:80:16:23\" - name: \"master2\" ipaddr: \"192.168.7.23\" macaddr: \"52:54:00:d5:1c:39\" workers: - name: \"worker0\" ipaddr: \"192.168.7.11\" macaddr: \"52:54:00:f4:26:a1\" - name: \"worker1\" ipaddr: \"192.168.7.12\" macaddr: \"52:54:00:82:90:00\" ppc64le: true ocp_bios: \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/dependencies/rhcos/4.4/latest/rhcos-4.4.9-ppc64le-metal.ppc64le.raw.gz\" ocp_initramfs: \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/dependencies/rhcos/4.4/latest/rhcos-4.4.9-ppc64le-installer-initramfs.ppc64le.img\" ocp_install_kernel: \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/dependencies/rhcos/4.4/latest/rhcos-4.4.9-ppc64le-installer-kernel-ppc64le\" ocp_client: \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp/stable-4.4/openshift-client-linux.tar.gz\" ocp_installer: \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp/stable-4.4/openshift-install-linux.tar.gz\" Run the playbook ansible-playbook -e @vars.yaml tasks/main.yml Create ignition configs mkdir ~/ocp4 cd ~/ocp4 Create a place to store your pull-secret mkdir -p ~/.openshift Visit try.openshift.com and select \"Bare Metal\". Download your pull secret and save it under ~/.openshift/pull-secret # ls -1 ~/.openshift/pull-secret /root/.openshift/pull-secret This playbook creates an ssh key for you; it's under ~/.ssh/helper_rsa . You can use this key or create/user another one if you wish. # ls -1 ~/.ssh/helper_rsa /root/.ssh/helper_rsa Note - If you want you use your own ssh key, please modify ~/.ssh/config to reference your key instead of the one deployed by the playbook Next, create an install-config.yaml file. Note - Make sure you update if your filenames or paths are different. cat <<EOF > install-config.yaml apiVersion: v1 baseDomain: example.com compute: - hyperthreading: Enabled name: worker replicas: 0 controlPlane: hyperthreading: Enabled name: master replicas: 3 metadata: name: ocp4 networking: clusterNetworks: - cidr: 10.254.0.0/16 hostPrefix: 24 networkType: OpenShiftSDN serviceNetwork: - 172.30.0.0/16 platform: none: {} pullSecret: '$(< ~/.openshift/pull-secret)' sshKey: '$(< ~/.ssh/helper_rsa.pub)' EOF Create the installation manifests openshift-install create manifests Edit the manifests/cluster-scheduler-02-config.yml Kubernetes manifest file to prevent Pods from being scheduled on the control plane machines by setting mastersSchedulable to false . $ sed -i 's/mastersSchedulable: true/mastersSchedulable: false/g' manifests/cluster-scheduler-02-config.yml It should look something like this after you edit it. $ cat manifests/cluster-scheduler-02-config.yml apiVersion: config.openshift.io/v1 kind: Scheduler metadata: creationTimestamp: null name: cluster spec: mastersSchedulable: false policy: name: \"\" status: {} Next, generate the ignition configs openshift-install create ignition-configs Finally, copy the ignition files in the ignition directory for the websever cp ~/ocp4/*.ign /var/www/html/ignition/ restorecon -vR /var/www/html/ chmod o+r /var/www/html/ignition/*.ign Boot the LPARs Boot the LPARs in the following order and ensure the LPARs perform DHCP boot Bootstrap Masters Workers Wait for Install openshift-install wait-for bootstrap-complete --log-level debug Finish Install First, login to your cluster export KUBECONFIG=/root/ocp4/auth/kubeconfig Your install may be waiting for worker nodes to get approved. Normally it's automated. However, sometimes this needs to be done manually. Check pending CSRs with the following command. oc get csr You can approve all pending CSRs in \"one shot\" with the following command oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{\"\\n\"}}{{end}}{{end}}' | xargs oc adm certificate approve You may have to run the command multiple times depending on how many workers you have and in what order they come in. Keep a watch on the CSRs by running the following command watch oc get csr Set the registry for your cluster First, you have to set the managementState to Managed for your cluster oc patch configs.imageregistry.operator.openshift.io cluster --type merge --patch '{\"spec\":{\"managementState\":\"Managed\"}}' For PoCs, using emptyDir is ok (to use PVs follow this doc) oc patch configs.imageregistry.operator.openshift.io cluster --type merge --patch '{\"spec\":{\"storage\":{\"emptyDir\":{}}}}' If you need to expose the registry, run this command oc patch configs.imageregistry.operator.openshift.io/cluster --type merge -p '{\"spec\":{\"defaultRoute\":true}}' Note - You can watch the operators running with oc get clusteroperators Login to the web console The OpenShift 4 web console will be running at https://console-openshift-console.apps.{{ dns.clusterid }}.{{ dns.domain }} (e.g. https://console-openshift-console.apps.ocp4.example.com) Username: kubeadmin Password: the output of cat /root/ocp4/auth/kubeadmin-password Note - You'll need to update your /etc/hosts settings if using private dhcp server running on the bastion node References: - Quickstart Guide - Power QuickStart Guide","title":"Ocp4 manual deployment dhcp"},{"location":"ocp4-upi-powervm/docs/ocp4-manual-deployment-dhcp/#create-masterworkerbootstrap-nodes","text":"Create the below PowerVM LPARS with empty disk volume attached (refer the documentation link for resource requirements) and note the MAC ID for each of the LPARs. bootstrap - 1 master - 3 worker - 2","title":"Create Master/Worker/Bootstrap Nodes"},{"location":"ocp4-upi-powervm/docs/ocp4-manual-deployment-dhcp/#create-and-setup-bastion-node","text":"Create RHEL 8.1 LPAR Login to the RHEL 8.1 LPAR and clone the OCP4 helpernode repo Use the following vars.yaml as a template and change the IP, network and related details according to your environment. --- disk: sda helper: name: \"helper\" ipaddr: \"192.168.7.77\" dns: domain: \"example.com\" clusterid: \"ocp4\" forwarder1: \"8.8.8.8\" forwarder2: \"8.8.4.4\" dhcp: router: \"192.168.7.1\" bcast: \"192.168.7.255\" netmask: \"255.255.255.0\" poolstart: \"192.168.7.10\" poolend: \"192.168.7.30\" ipid: \"192.168.7.0\" netmaskid: \"255.255.255.0\" bootstrap: name: \"bootstrap\" ipaddr: \"192.168.7.20\" macaddr: \"52:54:00:60:72:67\" masters: - name: \"master0\" ipaddr: \"192.168.7.21\" macaddr: \"52:54:00:e7:9d:67\" - name: \"master1\" ipaddr: \"192.168.7.22\" macaddr: \"52:54:00:80:16:23\" - name: \"master2\" ipaddr: \"192.168.7.23\" macaddr: \"52:54:00:d5:1c:39\" workers: - name: \"worker0\" ipaddr: \"192.168.7.11\" macaddr: \"52:54:00:f4:26:a1\" - name: \"worker1\" ipaddr: \"192.168.7.12\" macaddr: \"52:54:00:82:90:00\" ppc64le: true ocp_bios: \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/dependencies/rhcos/4.4/latest/rhcos-4.4.9-ppc64le-metal.ppc64le.raw.gz\" ocp_initramfs: \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/dependencies/rhcos/4.4/latest/rhcos-4.4.9-ppc64le-installer-initramfs.ppc64le.img\" ocp_install_kernel: \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/dependencies/rhcos/4.4/latest/rhcos-4.4.9-ppc64le-installer-kernel-ppc64le\" ocp_client: \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp/stable-4.4/openshift-client-linux.tar.gz\" ocp_installer: \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp/stable-4.4/openshift-install-linux.tar.gz\" Run the playbook ansible-playbook -e @vars.yaml tasks/main.yml Create ignition configs mkdir ~/ocp4 cd ~/ocp4 Create a place to store your pull-secret mkdir -p ~/.openshift Visit try.openshift.com and select \"Bare Metal\". Download your pull secret and save it under ~/.openshift/pull-secret # ls -1 ~/.openshift/pull-secret /root/.openshift/pull-secret This playbook creates an ssh key for you; it's under ~/.ssh/helper_rsa . You can use this key or create/user another one if you wish. # ls -1 ~/.ssh/helper_rsa /root/.ssh/helper_rsa Note - If you want you use your own ssh key, please modify ~/.ssh/config to reference your key instead of the one deployed by the playbook Next, create an install-config.yaml file. Note - Make sure you update if your filenames or paths are different. cat <<EOF > install-config.yaml apiVersion: v1 baseDomain: example.com compute: - hyperthreading: Enabled name: worker replicas: 0 controlPlane: hyperthreading: Enabled name: master replicas: 3 metadata: name: ocp4 networking: clusterNetworks: - cidr: 10.254.0.0/16 hostPrefix: 24 networkType: OpenShiftSDN serviceNetwork: - 172.30.0.0/16 platform: none: {} pullSecret: '$(< ~/.openshift/pull-secret)' sshKey: '$(< ~/.ssh/helper_rsa.pub)' EOF Create the installation manifests openshift-install create manifests Edit the manifests/cluster-scheduler-02-config.yml Kubernetes manifest file to prevent Pods from being scheduled on the control plane machines by setting mastersSchedulable to false . $ sed -i 's/mastersSchedulable: true/mastersSchedulable: false/g' manifests/cluster-scheduler-02-config.yml It should look something like this after you edit it. $ cat manifests/cluster-scheduler-02-config.yml apiVersion: config.openshift.io/v1 kind: Scheduler metadata: creationTimestamp: null name: cluster spec: mastersSchedulable: false policy: name: \"\" status: {} Next, generate the ignition configs openshift-install create ignition-configs Finally, copy the ignition files in the ignition directory for the websever cp ~/ocp4/*.ign /var/www/html/ignition/ restorecon -vR /var/www/html/ chmod o+r /var/www/html/ignition/*.ign","title":"Create and Setup Bastion Node"},{"location":"ocp4-upi-powervm/docs/ocp4-manual-deployment-dhcp/#boot-the-lpars","text":"Boot the LPARs in the following order and ensure the LPARs perform DHCP boot Bootstrap Masters Workers","title":"Boot the LPARs"},{"location":"ocp4-upi-powervm/docs/ocp4-manual-deployment-dhcp/#wait-for-install","text":"openshift-install wait-for bootstrap-complete --log-level debug","title":"Wait for Install"},{"location":"ocp4-upi-powervm/docs/ocp4-manual-deployment-dhcp/#finish-install","text":"First, login to your cluster export KUBECONFIG=/root/ocp4/auth/kubeconfig Your install may be waiting for worker nodes to get approved. Normally it's automated. However, sometimes this needs to be done manually. Check pending CSRs with the following command. oc get csr You can approve all pending CSRs in \"one shot\" with the following command oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{\"\\n\"}}{{end}}{{end}}' | xargs oc adm certificate approve You may have to run the command multiple times depending on how many workers you have and in what order they come in. Keep a watch on the CSRs by running the following command watch oc get csr Set the registry for your cluster First, you have to set the managementState to Managed for your cluster oc patch configs.imageregistry.operator.openshift.io cluster --type merge --patch '{\"spec\":{\"managementState\":\"Managed\"}}' For PoCs, using emptyDir is ok (to use PVs follow this doc) oc patch configs.imageregistry.operator.openshift.io cluster --type merge --patch '{\"spec\":{\"storage\":{\"emptyDir\":{}}}}' If you need to expose the registry, run this command oc patch configs.imageregistry.operator.openshift.io/cluster --type merge -p '{\"spec\":{\"defaultRoute\":true}}' Note - You can watch the operators running with oc get clusteroperators","title":"Finish Install"},{"location":"ocp4-upi-powervm/docs/ocp4-manual-deployment-dhcp/#login-to-the-web-console","text":"The OpenShift 4 web console will be running at https://console-openshift-console.apps.{{ dns.clusterid }}.{{ dns.domain }} (e.g. https://console-openshift-console.apps.ocp4.example.com) Username: kubeadmin Password: the output of cat /root/ocp4/auth/kubeadmin-password Note - You'll need to update your /etc/hosts settings if using private dhcp server running on the bastion node References: - Quickstart Guide - Power QuickStart Guide","title":"Login to the web console"},{"location":"ocp4-upi-powervm/docs/ocp_prereqs_powervc/","text":"PowerVC Prerequisites RHCOS and RHEL 8.X Images for OpenShift You'll need to create RedHat CoreOS (RHCOS) and RHEL 8.2 (or later) image in PowerVC. RHEL 8.x image is used by bastion node, and RHCOS image is used for boostrap, master and worker nodes. For RHEL image creation follow the steps mentioned in the following doc For RHCOS image creation, follow the steps mentioned in the following doc . Compute Templates You'll need to create compute templates for bastion, bootstrap, master and worker nodes. Following are the recommended LPAR configs that you can use when creating the compute templates for different type of nodes Bootstrap, Master - 2 vCPUs, 16GB RAM, 120 GB Disk. PowerVM LPARs by default uses SMT=8. So with 2vCPUs, the number of logical CPUs as seen by the Operating System will be 16 ( 2 vCPUs x 8 SMT ) This config is suitable for majority of the scenarios Worker - 2 vCPUs, 16GB RAM, 120 GB Disk Increase worker vCPUs, RAM and Disk based on application requirements Bastion - 2vCPUs, 16GB RAM, 200 GB Disk Increase worker vCPUs, RAM and Disk based on application requirements","title":"**PowerVC Prerequisites**"},{"location":"ocp4-upi-powervm/docs/ocp_prereqs_powervc/#powervc-prerequisites","text":"","title":"PowerVC Prerequisites"},{"location":"ocp4-upi-powervm/docs/ocp_prereqs_powervc/#rhcos-and-rhel-8x-images-for-openshift","text":"You'll need to create RedHat CoreOS (RHCOS) and RHEL 8.2 (or later) image in PowerVC. RHEL 8.x image is used by bastion node, and RHCOS image is used for boostrap, master and worker nodes. For RHEL image creation follow the steps mentioned in the following doc For RHCOS image creation, follow the steps mentioned in the following doc .","title":"RHCOS and RHEL 8.X Images for OpenShift"},{"location":"ocp4-upi-powervm/docs/ocp_prereqs_powervc/#compute-templates","text":"You'll need to create compute templates for bastion, bootstrap, master and worker nodes. Following are the recommended LPAR configs that you can use when creating the compute templates for different type of nodes Bootstrap, Master - 2 vCPUs, 16GB RAM, 120 GB Disk. PowerVM LPARs by default uses SMT=8. So with 2vCPUs, the number of logical CPUs as seen by the Operating System will be 16 ( 2 vCPUs x 8 SMT ) This config is suitable for majority of the scenarios Worker - 2 vCPUs, 16GB RAM, 120 GB Disk Increase worker vCPUs, RAM and Disk based on application requirements Bastion - 2vCPUs, 16GB RAM, 200 GB Disk Increase worker vCPUs, RAM and Disk based on application requirements","title":"Compute Templates"},{"location":"ocp4-upi-powervm/docs/quickstart/","text":"Installation Quickstart Installation Quickstart Download the Automation Code Setup Terraform Variables Start Install Post Install Delete Bootstrap Node Create API and Ingress DNS Records Cluster Access Using CLI Using Web UI Clean up Download the Automation Code You'll need to use git to clone the deployment code when working off the master branch $ git clone https://github.com/ocp-power-automation/ocp4-upi-powervm.git $ cd ocp4_upi_powervm All further instructions assumes you are in the code directory eg. ocp4-upi-powervm Setup Terraform Variables Update the var.tfvars based on your environment. Description of the variables are available in the following link . You can use environment variables for sensitive data that should not be saved to disk. $ set +o history $ export POWERVC_USERNAME=xxxxxxxxxxxxxxx $ export POWERVC_PASSWORD=xxxxxxxxxxxxxxx $ export RHEL_SUBS_USERNAME=xxxxxxxxxxxxxxx $ export RHEL_SUBS_PASSWORD=xxxxxxxxxxxxxxx $ set -o history Start Install Run the following commands from within the directory. $ terraform init $ terraform apply -var-file var.tfvars If using environment variables for sensitive data, then do the following, instead. $ terraform init $ terraform apply -var-file var.tfvars -var user_name=\"$POWERVC_USERNAME\" -var password=\"$POWERVC_PASSWORD\" -var rhel_subscription_username=\"$RHEL_SUBS_USERNAME\" -var rhel_subscription_password=\"$RHEL_SUBS_PASSWORD\" Now wait for the installation to complete. It may take around 40 mins to complete provisioning. On successful install cluster details will be printed as shown below. bastion_private_ip = 192.168.25.171 bastion_public_ip = 16.20.34.5 bastion_ssh_command = ssh -i data/id_rsa root@16.20.34.5 bootstrap_ip = 192.168.25.182 cluster_authentication_details = Cluster authentication details are available in 16.20.34.5 under ~/openstack-upi/auth cluster_id = test-cluster-9a4f etc_hosts_entries = 16.20.34.5 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com install_status = COMPLETED master_ips = [ \"192.168.25.147\", \"192.168.25.176\", ] oc_server_url = https://test-cluster-9a4f.mydomain.com:6443 storageclass_name = nfs-storage-provisioner web_console_url = https://console-openshift-console.apps.test-cluster-9a4f.mydomain.com worker_ips = [ \"192.168.25.220\", \"192.168.25.134\", ] When using wildcard domain like nip.io or xip.io then etc_host_entries is empty bastion_private_ip = 192.168.25.171 bastion_public_ip = 16.20.34.5 bastion_ssh_command = ssh -i data/id_rsa root@16.20.34.5 bootstrap_ip = 192.168.25.182 cluster_authentication_details = Cluster authentication details are available in 16.20.34.5 under ~/openstack-upi/auth cluster_id = test-cluster-9a4f etc_hosts_entries = install_status = COMPLETED master_ips = [ \"192.168.25.147\", \"192.168.25.176\", ] oc_server_url = https://test-cluster-9a4f.16.20.34.5.nip.io:6443 storageclass_name = nfs-storage-provisioner web_console_url = https://console-openshift-console.apps.test-cluster-9a4f.16.20.34.5.nip.io worker_ips = [ \"192.168.25.220\", \"192.168.25.134\", ] These details can be retrieved anytime by running the following command from the root folder of the code $ terraform output In case of any errors, you'll have to re-apply. Please refer to known issues to get more details on potential issues and workarounds. Post Install Delete Bootstrap Node Once the deployment is completed successfully, you can safely delete the bootstrap node. This step is optional but recommended so as to free up the resources used. Change the count value to 0 in bootstrap map variable and re-run the apply command. Eg: bootstrap = {instance_type = \"medium\", image_id = \"468863e6-4b33-4e8b-b2c5-c9ef9e6eedf4\", \"count\" = 0} Run command terraform apply -var-file var.tfvars Create API and Ingress DNS Records Please skip this section if your cluster_domain is one of the online wildcard DNS domains: nip.io, xip.io and sslip.io. For all other domains, you can use one of the following options. Add entries to your DNS server The general format is shown below: api.<cluster_id>. IN A <bastion_public_ip> *.apps.<cluster_id>. IN A <bastion_public_ip> You'll need bastion_public_ip and cluster_id . This is printed at the end of a successful install. Or you can retrieve it anytime by running terraform output from the install directory. For example, if bastion_public_ip = 16.20.34.5 and cluster_id = test-cluster-9a4f then the following DNS records will need to be added. api.test-cluster-9a4f. IN A 16.20.34.5 *.apps.test-cluster-9a4f. IN A 16.20.34.5 Add entries to your client system hosts file For Linux and Mac hosts file is located at /etc/hosts and for Windows it's located at c:\\Windows\\System32\\Drivers\\etc\\hosts . The general format is shown below: <bastion_public_ip> api.<cluster_id> <bastion_public_ip> console-openshift-console.apps.<cluster_id> <bastion_public_ip> integrated-oauth-server-openshift-authentication.apps.<cluster_id> <bastion_public_ip> oauth-openshift.apps.<cluster_id> <bastion_public_ip> prometheus-k8s-openshift-monitoring.apps.<cluster_id> <bastion_public_ip> grafana-openshift-monitoring.apps.<cluster_id> <bastion_public_ip> <app name>.apps.<cluster_id> You'll need etc_host_entries . This is printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. As an example, for the following etc_hosts_entries etc_hosts_entries = 16.20.34.5 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com just add the following entry to the hosts file ``` [existing entries in hosts file] 16.20.34.5 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com ``` Cluster Access OpenShift login credentials are in the bastion host and the location will be printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. [...] bastion_public_ip = 16.20.34.5 bastion_ssh_command = ssh -i data/id_rsa root@16.20.34.5 cluster_authentication_details = Cluster authentication details are available in 16.20.34.5 under ~/openstack-upi/auth [...] There are two files under ~/openstack-upi/auth - kubeconfig : can be used for CLI access - kubeadmin-password : Password for kubeadmin user which can be used for CLI, UI access Note : Ensure you securely store the OpenShift cluster access credentials. If desired delete the access details from the bastion node after securely storing the same. You can copy the access details to your local system $ scp -r -i data/id_rsa root@158.175.161.118:~/openstack-upi/auth/\\* . Using CLI OpenShift CLI oc can be downloaded from the following links. Use the one specific to your client system architecture. Mac OSX Linux (x86_64) Linux (ppc64le) Windows Download the specific file, extract it and place the binary in a directory that is on your PATH For more details check the following link The CLI login URL oc_server_url will be printed at the end of successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. [...] oc_server_url = https://test-cluster-9a4f.mydomain.com:6443 [...] In order to login the cluster you can use the oc login <oc_server_url> -u kubeadmin -p <kubeadmin-password> Example: $ oc login https://test-cluster-9a4f.mydomain.com:6443 -u kubeadmin -p $(cat kubeadmin-password) You can also use the kubeconfig file $ export KUBECONFIG=$(pwd)/kubeconfig $ oc cluster-info Kubernetes master is running at https://test-cluster-9a4f.mydomain.com:6443 To further debug and diagnose cluster problems, use 'kubectl cluster-info dump' $ oc get nodes NAME STATUS ROLES AGE VERSION master-0 Ready master 13h v1.18.3+b74c5ed master-1 Ready master 13h v1.18.3+b74c5ed master-2 Ready master 13h v1.18.3+b74c5ed worker-0 Ready worker 13h v1.18.3+b74c5ed worker-1 Ready worker 13h v1.18.3+b74c5ed Note: The OpenShift command-line client oc is already configured on the bastion node with kubeconfig placed at ~/.kube/config . Using Web UI The web console URL will be printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. [...] web_console_url = https://console-openshift-console.apps.test-cluster-9a4f.mydomain.com [...] Open this URL in your browser and login with user kubeadmin and password mentioned in the kubeadmin-password file. Clean up To destroy after you are done using the cluster you can run command terraform destroy -var-file var.tfvars to make sure that all resources are properly cleaned up. Do not manually clean up your environment unless both of the following are true: You know what you are doing Something went wrong with an automated deletion.","title":"Installation Quickstart"},{"location":"ocp4-upi-powervm/docs/quickstart/#installation-quickstart","text":"Installation Quickstart Download the Automation Code Setup Terraform Variables Start Install Post Install Delete Bootstrap Node Create API and Ingress DNS Records Cluster Access Using CLI Using Web UI Clean up","title":"Installation Quickstart"},{"location":"ocp4-upi-powervm/docs/quickstart/#download-the-automation-code","text":"You'll need to use git to clone the deployment code when working off the master branch $ git clone https://github.com/ocp-power-automation/ocp4-upi-powervm.git $ cd ocp4_upi_powervm All further instructions assumes you are in the code directory eg. ocp4-upi-powervm","title":"Download the Automation Code"},{"location":"ocp4-upi-powervm/docs/quickstart/#setup-terraform-variables","text":"Update the var.tfvars based on your environment. Description of the variables are available in the following link . You can use environment variables for sensitive data that should not be saved to disk. $ set +o history $ export POWERVC_USERNAME=xxxxxxxxxxxxxxx $ export POWERVC_PASSWORD=xxxxxxxxxxxxxxx $ export RHEL_SUBS_USERNAME=xxxxxxxxxxxxxxx $ export RHEL_SUBS_PASSWORD=xxxxxxxxxxxxxxx $ set -o history","title":"Setup Terraform Variables"},{"location":"ocp4-upi-powervm/docs/quickstart/#start-install","text":"Run the following commands from within the directory. $ terraform init $ terraform apply -var-file var.tfvars If using environment variables for sensitive data, then do the following, instead. $ terraform init $ terraform apply -var-file var.tfvars -var user_name=\"$POWERVC_USERNAME\" -var password=\"$POWERVC_PASSWORD\" -var rhel_subscription_username=\"$RHEL_SUBS_USERNAME\" -var rhel_subscription_password=\"$RHEL_SUBS_PASSWORD\" Now wait for the installation to complete. It may take around 40 mins to complete provisioning. On successful install cluster details will be printed as shown below. bastion_private_ip = 192.168.25.171 bastion_public_ip = 16.20.34.5 bastion_ssh_command = ssh -i data/id_rsa root@16.20.34.5 bootstrap_ip = 192.168.25.182 cluster_authentication_details = Cluster authentication details are available in 16.20.34.5 under ~/openstack-upi/auth cluster_id = test-cluster-9a4f etc_hosts_entries = 16.20.34.5 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com install_status = COMPLETED master_ips = [ \"192.168.25.147\", \"192.168.25.176\", ] oc_server_url = https://test-cluster-9a4f.mydomain.com:6443 storageclass_name = nfs-storage-provisioner web_console_url = https://console-openshift-console.apps.test-cluster-9a4f.mydomain.com worker_ips = [ \"192.168.25.220\", \"192.168.25.134\", ] When using wildcard domain like nip.io or xip.io then etc_host_entries is empty bastion_private_ip = 192.168.25.171 bastion_public_ip = 16.20.34.5 bastion_ssh_command = ssh -i data/id_rsa root@16.20.34.5 bootstrap_ip = 192.168.25.182 cluster_authentication_details = Cluster authentication details are available in 16.20.34.5 under ~/openstack-upi/auth cluster_id = test-cluster-9a4f etc_hosts_entries = install_status = COMPLETED master_ips = [ \"192.168.25.147\", \"192.168.25.176\", ] oc_server_url = https://test-cluster-9a4f.16.20.34.5.nip.io:6443 storageclass_name = nfs-storage-provisioner web_console_url = https://console-openshift-console.apps.test-cluster-9a4f.16.20.34.5.nip.io worker_ips = [ \"192.168.25.220\", \"192.168.25.134\", ] These details can be retrieved anytime by running the following command from the root folder of the code $ terraform output In case of any errors, you'll have to re-apply. Please refer to known issues to get more details on potential issues and workarounds.","title":"Start Install"},{"location":"ocp4-upi-powervm/docs/quickstart/#post-install","text":"","title":"Post Install"},{"location":"ocp4-upi-powervm/docs/quickstart/#delete-bootstrap-node","text":"Once the deployment is completed successfully, you can safely delete the bootstrap node. This step is optional but recommended so as to free up the resources used. Change the count value to 0 in bootstrap map variable and re-run the apply command. Eg: bootstrap = {instance_type = \"medium\", image_id = \"468863e6-4b33-4e8b-b2c5-c9ef9e6eedf4\", \"count\" = 0} Run command terraform apply -var-file var.tfvars","title":"Delete Bootstrap Node"},{"location":"ocp4-upi-powervm/docs/quickstart/#create-api-and-ingress-dns-records","text":"Please skip this section if your cluster_domain is one of the online wildcard DNS domains: nip.io, xip.io and sslip.io. For all other domains, you can use one of the following options. Add entries to your DNS server The general format is shown below: api.<cluster_id>. IN A <bastion_public_ip> *.apps.<cluster_id>. IN A <bastion_public_ip> You'll need bastion_public_ip and cluster_id . This is printed at the end of a successful install. Or you can retrieve it anytime by running terraform output from the install directory. For example, if bastion_public_ip = 16.20.34.5 and cluster_id = test-cluster-9a4f then the following DNS records will need to be added. api.test-cluster-9a4f. IN A 16.20.34.5 *.apps.test-cluster-9a4f. IN A 16.20.34.5 Add entries to your client system hosts file For Linux and Mac hosts file is located at /etc/hosts and for Windows it's located at c:\\Windows\\System32\\Drivers\\etc\\hosts . The general format is shown below: <bastion_public_ip> api.<cluster_id> <bastion_public_ip> console-openshift-console.apps.<cluster_id> <bastion_public_ip> integrated-oauth-server-openshift-authentication.apps.<cluster_id> <bastion_public_ip> oauth-openshift.apps.<cluster_id> <bastion_public_ip> prometheus-k8s-openshift-monitoring.apps.<cluster_id> <bastion_public_ip> grafana-openshift-monitoring.apps.<cluster_id> <bastion_public_ip> <app name>.apps.<cluster_id> You'll need etc_host_entries . This is printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. As an example, for the following etc_hosts_entries etc_hosts_entries = 16.20.34.5 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com just add the following entry to the hosts file ``` [existing entries in hosts file] 16.20.34.5 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com ```","title":"Create API and Ingress DNS Records"},{"location":"ocp4-upi-powervm/docs/quickstart/#cluster-access","text":"OpenShift login credentials are in the bastion host and the location will be printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. [...] bastion_public_ip = 16.20.34.5 bastion_ssh_command = ssh -i data/id_rsa root@16.20.34.5 cluster_authentication_details = Cluster authentication details are available in 16.20.34.5 under ~/openstack-upi/auth [...] There are two files under ~/openstack-upi/auth - kubeconfig : can be used for CLI access - kubeadmin-password : Password for kubeadmin user which can be used for CLI, UI access Note : Ensure you securely store the OpenShift cluster access credentials. If desired delete the access details from the bastion node after securely storing the same. You can copy the access details to your local system $ scp -r -i data/id_rsa root@158.175.161.118:~/openstack-upi/auth/\\* .","title":"Cluster Access"},{"location":"ocp4-upi-powervm/docs/quickstart/#using-cli","text":"OpenShift CLI oc can be downloaded from the following links. Use the one specific to your client system architecture. Mac OSX Linux (x86_64) Linux (ppc64le) Windows Download the specific file, extract it and place the binary in a directory that is on your PATH For more details check the following link The CLI login URL oc_server_url will be printed at the end of successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. [...] oc_server_url = https://test-cluster-9a4f.mydomain.com:6443 [...] In order to login the cluster you can use the oc login <oc_server_url> -u kubeadmin -p <kubeadmin-password> Example: $ oc login https://test-cluster-9a4f.mydomain.com:6443 -u kubeadmin -p $(cat kubeadmin-password) You can also use the kubeconfig file $ export KUBECONFIG=$(pwd)/kubeconfig $ oc cluster-info Kubernetes master is running at https://test-cluster-9a4f.mydomain.com:6443 To further debug and diagnose cluster problems, use 'kubectl cluster-info dump' $ oc get nodes NAME STATUS ROLES AGE VERSION master-0 Ready master 13h v1.18.3+b74c5ed master-1 Ready master 13h v1.18.3+b74c5ed master-2 Ready master 13h v1.18.3+b74c5ed worker-0 Ready worker 13h v1.18.3+b74c5ed worker-1 Ready worker 13h v1.18.3+b74c5ed Note: The OpenShift command-line client oc is already configured on the bastion node with kubeconfig placed at ~/.kube/config .","title":"Using CLI"},{"location":"ocp4-upi-powervm/docs/quickstart/#using-web-ui","text":"The web console URL will be printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. [...] web_console_url = https://console-openshift-console.apps.test-cluster-9a4f.mydomain.com [...] Open this URL in your browser and login with user kubeadmin and password mentioned in the kubeadmin-password file.","title":"Using Web UI"},{"location":"ocp4-upi-powervm/docs/quickstart/#clean-up","text":"To destroy after you are done using the cluster you can run command terraform destroy -var-file var.tfvars to make sure that all resources are properly cleaned up. Do not manually clean up your environment unless both of the following are true: You know what you are doing Something went wrong with an automated deletion.","title":"Clean up"},{"location":"ocp4-upi-powervm/docs/rhcos-image-creation/","text":"Introduction Option-1 Option-2 Introduction Depending on your environment you can follow one of the options to create RHCOS (CoreOS) image in PowerVC Option-1 Download the RHCOS image from the following link on a system with public internet access. You'll need a way to transfer this image to a RHEL VM that you'll create in the next step. Login to PowerVC and create a RHEL 8.x VM having an additional empty volume with minimum size of 120G. Please make a note of the new volume name . Login to the VM and execute the following steps Install wget , qemu-img , parted and gzip packages Transfer the downloaded RHCOS image to this VM Extract the image $ gunzip rhcos-openstack.ppc64le.qcow2.gz Convert the CoreOS qcow2 image to raw image $ qemu-img convert -f qcow2 -O raw rhcos-openstack.ppc64le.qcow2 rhcos-latest.raw Identify the disk device representing the additional empty volume attached to the VM $ disk_device_list=$(sudo parted -l 2>&1 | grep -E -v \"$readonly\" | grep -E -i \"ERROR:\" |cut -f2 -d: | grep -v \"Can't\" | xargs -i echo \"Disk.{}:|\" | xargs echo | tr -d ' ' | rev | cut -c2- | rev) $ empty_disk_device=$(sudo fdisk -l | grep -E \"$disk_list\" | sort -k5nr | head -n 1 | tail -n1 | cut -f1 -d: | cut -f2 -d' ') $ echo \"$empty_disk_device\"' Dump the raw image to the newly added disk $ dd if=rhcos-latest.raw of=${empty_disk_device} bs=4M where ${empty_disk_device} is the device representing the attached volume Detach the volume, from the VM Go to PowerVC UI->images and select create for creating a new image Specify image name and choose PowerVM for Hypervisor type, RHEL for Operating system and littleEndian for Endianness Select Add Volume and search for the specific volume name (where you dd-ed the RHCOS image ) and set Boot set to yes. Create the image by clicking on create Option-2 Creating and importing RHCOS OVA image Download the RHCOS image from the following link on a system with public internet access. You'll need a way to transfer this image to a RHEL VM that you'll create in the next step. Login to PowerVC and create a RHEL 8.x VM Use the script https://github.com/ocp-power-automation/infra/blob/master/scripts/images/convert_qcow2_ova.py and convert the RHCOS qcow2 image to an OVA formatted image. Follow the steps mentioned in PowerVC docs to import the OVA image.","title":"Rhcos image creation"},{"location":"ocp4-upi-powervm/docs/rhcos-image-creation/#introduction","text":"Depending on your environment you can follow one of the options to create RHCOS (CoreOS) image in PowerVC","title":"Introduction"},{"location":"ocp4-upi-powervm/docs/rhcos-image-creation/#option-1","text":"Download the RHCOS image from the following link on a system with public internet access. You'll need a way to transfer this image to a RHEL VM that you'll create in the next step. Login to PowerVC and create a RHEL 8.x VM having an additional empty volume with minimum size of 120G. Please make a note of the new volume name . Login to the VM and execute the following steps Install wget , qemu-img , parted and gzip packages Transfer the downloaded RHCOS image to this VM Extract the image $ gunzip rhcos-openstack.ppc64le.qcow2.gz Convert the CoreOS qcow2 image to raw image $ qemu-img convert -f qcow2 -O raw rhcos-openstack.ppc64le.qcow2 rhcos-latest.raw Identify the disk device representing the additional empty volume attached to the VM $ disk_device_list=$(sudo parted -l 2>&1 | grep -E -v \"$readonly\" | grep -E -i \"ERROR:\" |cut -f2 -d: | grep -v \"Can't\" | xargs -i echo \"Disk.{}:|\" | xargs echo | tr -d ' ' | rev | cut -c2- | rev) $ empty_disk_device=$(sudo fdisk -l | grep -E \"$disk_list\" | sort -k5nr | head -n 1 | tail -n1 | cut -f1 -d: | cut -f2 -d' ') $ echo \"$empty_disk_device\"' Dump the raw image to the newly added disk $ dd if=rhcos-latest.raw of=${empty_disk_device} bs=4M where ${empty_disk_device} is the device representing the attached volume Detach the volume, from the VM Go to PowerVC UI->images and select create for creating a new image Specify image name and choose PowerVM for Hypervisor type, RHEL for Operating system and littleEndian for Endianness Select Add Volume and search for the specific volume name (where you dd-ed the RHCOS image ) and set Boot set to yes. Create the image by clicking on create","title":"Option-1"},{"location":"ocp4-upi-powervm/docs/rhcos-image-creation/#option-2","text":"Creating and importing RHCOS OVA image Download the RHCOS image from the following link on a system with public internet access. You'll need a way to transfer this image to a RHEL VM that you'll create in the next step. Login to PowerVC and create a RHEL 8.x VM Use the script https://github.com/ocp-power-automation/infra/blob/master/scripts/images/convert_qcow2_ova.py and convert the RHCOS qcow2 image to an OVA formatted image. Follow the steps mentioned in PowerVC docs to import the OVA image.","title":"Option-2"},{"location":"ocp4-upi-powervm/docs/var.tfvars-doc/","text":"How to use var.tfvars How to use var.tfvars Introduction PowerVC Details OpenShift Cluster Details OpenShift Installation Details Misc Customizations Introduction This guide gives an overview of the various terraform variables that are used for the deployment. The default values are set in variables.tf PowerVC Details These set of variables specify the PowerVC details. auth_url = \"<https://<HOSTNAME>:5000/v3/>\" user_name = \"<powervc-login-user-name>\" password = \"<powervc-login-user-password>\" tenant_name = \"<tenant_name>\" domain_name = \"Default\" This variable specifies the network that will be used by the VMs network_name = \"<network_name>\" This variable specifies the availability zone (PowerVC Host Group) in which to create the VMs. Leave it empty to use the \"default\" availability zone. openstack_availability_zone = \"\" OpenShift Cluster Details These set of variables specify the cluster capacity. bastion = {instance_type = \"<bastion-compute-template>\", image_id = \"<image-uuid-rhel>\"} bootstrap = {instance_type = \"<bootstrap-compute-template>\", image_id = \"<image-uuid-rhcos>\", \"count\" = 1} master = {instance_type = \"<master-compute-template>\", image_id = \"<image-uuid-rhcos>\", \"count\" = 3} worker = {instance_type = \"<worker-compute-template>\", image_id = \"<image-uuid-rhcos>\", \"count\" = 2} instance_type is the compute template to be used and image_id is the image UUID. count specifies the number of VMs that should be created for each type. You can optionally set worker count value to 0 in which case all the cluster pods will be running on the master/supervisor nodes. Ensure you use proper sizing for master/supervisor nodes to avoid resource starvation for containers. These set of variables specify the username and the SSH key to be used for accessing the bastion node. rhel_username = \"root\" public_key_file = \"data/id_rsa.pub\" private_key_file = \"data/id_rsa\" Please note that only OpenSSH formatted keys are supported. Refer to the following links for instructions on creating SSH key based on your platform. - Windows 10 - https://phoenixnap.com/kb/generate-ssh-key-windows-10 - Mac OSX - https://www.techrepublic.com/article/how-to-generate-ssh-keys-on-macos-mojave/ - Linux - https://www.siteground.com/kb/generate_ssh_key_in_linux/ Create the SSH key-pair and keep it under the data directory These set of variables specify the RHEL subscription details. This is sensitive data, and if you don't want to save it on disk, use environment variables RHEL_SUBS_USERNAME and RHEL_SUBS_PASSWORD and pass them to terraform apply command as shown in the Quickstart guide . rhel_subscription_username = \"user@test.com\" rhel_subscription_password = \"mypassword\" OpenShift Installation Details These variables specify the URL for the OpenShift installer and client binaries. Change the URL to the specific pre-release version that you want to install on PowerVS. Reference link - https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview openshift_install_tarball = \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview/latest/openshift-install-linux.tar.gz\" openshift_client_tarball = \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview/latest/openshift-client-linux.tar.gz\" This variable specifies the OpenShift pull secret. This is available from the following link - https://cloud.redhat.com/openshift/install/power/user-provisioned Download the secret and copy it to data/pull-secret.txt . pull_secret_file = \"data/pull-secret.txt\" These variables specifies the OpenShift cluster domain details. Edit it as per your requirements. cluster_domain = \"ibm.com\" cluster_id_prefix = \"test-ocp\" cluster_id = \"\" Set the cluster_domain to nip.io , xip.io or sslip.io if you prefer using online wildcard domains. Default is ibm.com . The cluster_id_prefix should not be more than 8 characters. Nodes are pre-fixed with this value. Default value is test-ocp A random value will be used for cluster_id if not set. The total length of cluster_id_prefix . cluster_id should not exceed 14 characters. Misc Customizations These variables provides miscellaneous customizations. For common usage scenarios these are not required and should be left unchanged. The following variable is used to set the network adapter type for the VMs. By default the VMs will use SEA. If SRIOV is required then uncomment the variable network_type = \"SRIOV\" The following variable is used to specify the PowerVC Storage Connectivity Group (SCG). Empty value will use the default SCG scg_id = \"\" The following variables can be used for disconnected install by using a local mirror registry on the bastion node. enable_local_registry = false #Set to true to enable usage of local registry for restricted network install. local_registry_image = \"docker.io/ibmcom/registry-ppc64le:2.6.2.5\" ocp_release_tag = \"4.4.9-ppc64le\" ocp_release_name = \"ocp-release\" This variable can be used for trying out custom OpenShift install image for development use. release_image_override = \"\" These variables specify the ansible playbooks that are used for OpenShift install and post-install customizations. helpernode_repo = \"https://github.com/RedHatOfficial/ocp4-helpernode\" helpernode_tag = \"5eab3db53976bb16be582f2edc2de02f7510050d\" install_playbook_repo = \"https://github.com/ocp-power-automation/ocp4-playbooks\" install_playbook_tag = \"02a598faa332aa2c3d53e8edd0e840440ff74bd5\" These variables can be used when debugging ansible playbooks installer_log_level = \"info\" ansible_extra_options = \"-v\" This variable specifies the external DNS servers to forward DNS queries that cannot be resolved locally. dns_forwarders = \"1.1.1.1; 9.9.9.9\" List of kernel arguments for the cluster nodes. Note that this will be applied after the cluster is installed and all the nodes are in Ready status. rhcos_kernel_options = [] Example 1 rhcos_kernel_options = [\"slub_max_order=0\",\"loglevel=7\"] These are NTP specific variables that are used for time-synchronization in the OpenShift cluster. chrony_config = true chrony_config_servers = [ {server = \"0.centos.pool.ntp.org\", options = \"iburst\"}, {server = \"1.centos.pool.ntp.org\", options = \"iburst\"} ] These set of variables are specific for cluster wide proxy configuration. Public internet access for the OpenShift cluster nodes is via Squid proxy deployed on the bastion. setup_squid_proxy = false If you have a separate proxy, and don't want to set the Squid proxy on bastion then use the following variables. setup_squid_proxy = false proxy = {server = \"hostname_or_ip\", port = \"3128\", user = \"pxuser\", password = \"pxpassword\"} Except server all other attributes are optional. Default port is 3128 with unauthenticated access. The following variable allows using RAM disk for etcd. This is not meant for production use cases mount_etcd_ramdisk = false These variables specify details about NFS storage that is setup by default on the bastion server. storage_type = \"nfs\" volume_size = \"300\" # Value in GB volume_storage_template = \"\" The following variables are specific to upgrading an existing installation. upgrade_version = \"\" upgrade_channel = \"\" #(stable-4.x, fast-4.x, candidate-4.x) eg. stable-4.5 upgrade_pause_time = \"90\" upgrade_delay_time = \"600\"","title":"How to use var.tfvars"},{"location":"ocp4-upi-powervm/docs/var.tfvars-doc/#how-to-use-vartfvars","text":"How to use var.tfvars Introduction PowerVC Details OpenShift Cluster Details OpenShift Installation Details Misc Customizations","title":"How to use var.tfvars"},{"location":"ocp4-upi-powervm/docs/var.tfvars-doc/#introduction","text":"This guide gives an overview of the various terraform variables that are used for the deployment. The default values are set in variables.tf","title":"Introduction"},{"location":"ocp4-upi-powervm/docs/var.tfvars-doc/#powervc-details","text":"These set of variables specify the PowerVC details. auth_url = \"<https://<HOSTNAME>:5000/v3/>\" user_name = \"<powervc-login-user-name>\" password = \"<powervc-login-user-password>\" tenant_name = \"<tenant_name>\" domain_name = \"Default\" This variable specifies the network that will be used by the VMs network_name = \"<network_name>\" This variable specifies the availability zone (PowerVC Host Group) in which to create the VMs. Leave it empty to use the \"default\" availability zone. openstack_availability_zone = \"\"","title":"PowerVC Details"},{"location":"ocp4-upi-powervm/docs/var.tfvars-doc/#openshift-cluster-details","text":"These set of variables specify the cluster capacity. bastion = {instance_type = \"<bastion-compute-template>\", image_id = \"<image-uuid-rhel>\"} bootstrap = {instance_type = \"<bootstrap-compute-template>\", image_id = \"<image-uuid-rhcos>\", \"count\" = 1} master = {instance_type = \"<master-compute-template>\", image_id = \"<image-uuid-rhcos>\", \"count\" = 3} worker = {instance_type = \"<worker-compute-template>\", image_id = \"<image-uuid-rhcos>\", \"count\" = 2} instance_type is the compute template to be used and image_id is the image UUID. count specifies the number of VMs that should be created for each type. You can optionally set worker count value to 0 in which case all the cluster pods will be running on the master/supervisor nodes. Ensure you use proper sizing for master/supervisor nodes to avoid resource starvation for containers. These set of variables specify the username and the SSH key to be used for accessing the bastion node. rhel_username = \"root\" public_key_file = \"data/id_rsa.pub\" private_key_file = \"data/id_rsa\" Please note that only OpenSSH formatted keys are supported. Refer to the following links for instructions on creating SSH key based on your platform. - Windows 10 - https://phoenixnap.com/kb/generate-ssh-key-windows-10 - Mac OSX - https://www.techrepublic.com/article/how-to-generate-ssh-keys-on-macos-mojave/ - Linux - https://www.siteground.com/kb/generate_ssh_key_in_linux/ Create the SSH key-pair and keep it under the data directory These set of variables specify the RHEL subscription details. This is sensitive data, and if you don't want to save it on disk, use environment variables RHEL_SUBS_USERNAME and RHEL_SUBS_PASSWORD and pass them to terraform apply command as shown in the Quickstart guide . rhel_subscription_username = \"user@test.com\" rhel_subscription_password = \"mypassword\"","title":"OpenShift Cluster Details"},{"location":"ocp4-upi-powervm/docs/var.tfvars-doc/#openshift-installation-details","text":"These variables specify the URL for the OpenShift installer and client binaries. Change the URL to the specific pre-release version that you want to install on PowerVS. Reference link - https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview openshift_install_tarball = \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview/latest/openshift-install-linux.tar.gz\" openshift_client_tarball = \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview/latest/openshift-client-linux.tar.gz\" This variable specifies the OpenShift pull secret. This is available from the following link - https://cloud.redhat.com/openshift/install/power/user-provisioned Download the secret and copy it to data/pull-secret.txt . pull_secret_file = \"data/pull-secret.txt\" These variables specifies the OpenShift cluster domain details. Edit it as per your requirements. cluster_domain = \"ibm.com\" cluster_id_prefix = \"test-ocp\" cluster_id = \"\" Set the cluster_domain to nip.io , xip.io or sslip.io if you prefer using online wildcard domains. Default is ibm.com . The cluster_id_prefix should not be more than 8 characters. Nodes are pre-fixed with this value. Default value is test-ocp A random value will be used for cluster_id if not set. The total length of cluster_id_prefix . cluster_id should not exceed 14 characters.","title":"OpenShift Installation Details"},{"location":"ocp4-upi-powervm/docs/var.tfvars-doc/#misc-customizations","text":"These variables provides miscellaneous customizations. For common usage scenarios these are not required and should be left unchanged. The following variable is used to set the network adapter type for the VMs. By default the VMs will use SEA. If SRIOV is required then uncomment the variable network_type = \"SRIOV\" The following variable is used to specify the PowerVC Storage Connectivity Group (SCG). Empty value will use the default SCG scg_id = \"\" The following variables can be used for disconnected install by using a local mirror registry on the bastion node. enable_local_registry = false #Set to true to enable usage of local registry for restricted network install. local_registry_image = \"docker.io/ibmcom/registry-ppc64le:2.6.2.5\" ocp_release_tag = \"4.4.9-ppc64le\" ocp_release_name = \"ocp-release\" This variable can be used for trying out custom OpenShift install image for development use. release_image_override = \"\" These variables specify the ansible playbooks that are used for OpenShift install and post-install customizations. helpernode_repo = \"https://github.com/RedHatOfficial/ocp4-helpernode\" helpernode_tag = \"5eab3db53976bb16be582f2edc2de02f7510050d\" install_playbook_repo = \"https://github.com/ocp-power-automation/ocp4-playbooks\" install_playbook_tag = \"02a598faa332aa2c3d53e8edd0e840440ff74bd5\" These variables can be used when debugging ansible playbooks installer_log_level = \"info\" ansible_extra_options = \"-v\" This variable specifies the external DNS servers to forward DNS queries that cannot be resolved locally. dns_forwarders = \"1.1.1.1; 9.9.9.9\" List of kernel arguments for the cluster nodes. Note that this will be applied after the cluster is installed and all the nodes are in Ready status. rhcos_kernel_options = [] Example 1 rhcos_kernel_options = [\"slub_max_order=0\",\"loglevel=7\"] These are NTP specific variables that are used for time-synchronization in the OpenShift cluster. chrony_config = true chrony_config_servers = [ {server = \"0.centos.pool.ntp.org\", options = \"iburst\"}, {server = \"1.centos.pool.ntp.org\", options = \"iburst\"} ] These set of variables are specific for cluster wide proxy configuration. Public internet access for the OpenShift cluster nodes is via Squid proxy deployed on the bastion. setup_squid_proxy = false If you have a separate proxy, and don't want to set the Squid proxy on bastion then use the following variables. setup_squid_proxy = false proxy = {server = \"hostname_or_ip\", port = \"3128\", user = \"pxuser\", password = \"pxpassword\"} Except server all other attributes are optional. Default port is 3128 with unauthenticated access. The following variable allows using RAM disk for etcd. This is not meant for production use cases mount_etcd_ramdisk = false These variables specify details about NFS storage that is setup by default on the bastion server. storage_type = \"nfs\" volume_size = \"300\" # Value in GB volume_storage_template = \"\" The following variables are specific to upgrading an existing installation. upgrade_version = \"\" upgrade_channel = \"\" #(stable-4.x, fast-4.x, candidate-4.x) eg. stable-4.5 upgrade_pause_time = \"90\" upgrade_delay_time = \"600\"","title":"Misc Customizations"},{"location":"ocp4-upi-powervs/","text":"Table of Contents Table of Contents Introduction Automation Host Prerequisites PowerVS Prerequisites OCP Install Contributing Introduction The ocp4-upi-powervs project provides Terraform based automation code to help the deployment of OpenShift Container Platform (OCP) 4.x on IBM\u00ae Power Systems\u2122 Virtual Server on IBM Cloud . This project leverages the helpernode ansible playbook internally for OCP deployment on IBM Power Systems Virtual Servers (PowerVS). :heavy_exclamation_mark: For bugs/enhancement requests etc. please open a GitHub issue For general PowerVS usage instructions please refer to the following links: - https://cloud.ibm.com/docs/power-iaas?topic=power-iaas-getting-started - https://www.youtube.com/watch?v=RywSfXT_LLs - https://www.youtube.com/playlist?list=PLVrJaTKVPbKM_9HU8fm4QsklgzLGUwFpv :information_source: The main branch must be used with latest OCP pre-release versions only. For stable releases please checkout specific release branches - { release-4.5 , release-4.6 ...} and follow the docs in the specific release branches. Automation Host Prerequisites The automation needs to run from a system with internet access. This could be your laptop or a VM with public internet connectivity. This automation code have been tested on the following Operating Systems: - Mac OSX (Darwin) - Linux (x86_64) - Windows 10 Follow the guide to complete the prerequisites. PowerVS Prerequisites Follow the guide to complete the PowerVS prerequisites. OCP Install Follow the quickstart guide for OCP installation on PowerVS. Contributing Please see the contributing doc for more details. PRs are most welcome !!","title":"Deploying OpenShift on IBM Cloud Power Virtual Servers"},{"location":"ocp4-upi-powervs/#table-of-contents","text":"Table of Contents Introduction Automation Host Prerequisites PowerVS Prerequisites OCP Install Contributing","title":"Table of Contents"},{"location":"ocp4-upi-powervs/#introduction","text":"The ocp4-upi-powervs project provides Terraform based automation code to help the deployment of OpenShift Container Platform (OCP) 4.x on IBM\u00ae Power Systems\u2122 Virtual Server on IBM Cloud . This project leverages the helpernode ansible playbook internally for OCP deployment on IBM Power Systems Virtual Servers (PowerVS). :heavy_exclamation_mark: For bugs/enhancement requests etc. please open a GitHub issue For general PowerVS usage instructions please refer to the following links: - https://cloud.ibm.com/docs/power-iaas?topic=power-iaas-getting-started - https://www.youtube.com/watch?v=RywSfXT_LLs - https://www.youtube.com/playlist?list=PLVrJaTKVPbKM_9HU8fm4QsklgzLGUwFpv :information_source: The main branch must be used with latest OCP pre-release versions only. For stable releases please checkout specific release branches - { release-4.5 , release-4.6 ...} and follow the docs in the specific release branches.","title":"Introduction"},{"location":"ocp4-upi-powervs/#automation-host-prerequisites","text":"The automation needs to run from a system with internet access. This could be your laptop or a VM with public internet connectivity. This automation code have been tested on the following Operating Systems: - Mac OSX (Darwin) - Linux (x86_64) - Windows 10 Follow the guide to complete the prerequisites.","title":"Automation Host Prerequisites"},{"location":"ocp4-upi-powervs/#powervs-prerequisites","text":"Follow the guide to complete the PowerVS prerequisites.","title":"PowerVS Prerequisites"},{"location":"ocp4-upi-powervs/#ocp-install","text":"Follow the quickstart guide for OCP installation on PowerVS.","title":"OCP Install"},{"location":"ocp4-upi-powervs/#contributing","text":"Please see the contributing doc for more details. PRs are most welcome !!","title":"Contributing"},{"location":"ocp4-upi-powervs/CODE_OF_CONDUCT/","text":"Contributor Covenant Code of Conduct Our Pledge We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. Our Standards Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Enforcement Responsibilities Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate. Scope This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement. Contact with repository owners. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident. Enforcement Guidelines Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct: 1. Correction Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested. 2. Warning Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban. 3. Temporary Ban Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban. 4. Permanent Ban Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html. Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder .","title":"Contributor Covenant Code of Conduct"},{"location":"ocp4-upi-powervs/CODE_OF_CONDUCT/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"ocp4-upi-powervs/CODE_OF_CONDUCT/#our-pledge","text":"We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.","title":"Our Pledge"},{"location":"ocp4-upi-powervs/CODE_OF_CONDUCT/#our-standards","text":"Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"ocp4-upi-powervs/CODE_OF_CONDUCT/#enforcement-responsibilities","text":"Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.","title":"Enforcement Responsibilities"},{"location":"ocp4-upi-powervs/CODE_OF_CONDUCT/#scope","text":"This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.","title":"Scope"},{"location":"ocp4-upi-powervs/CODE_OF_CONDUCT/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement. Contact with repository owners. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident.","title":"Enforcement"},{"location":"ocp4-upi-powervs/CODE_OF_CONDUCT/#enforcement-guidelines","text":"Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:","title":"Enforcement Guidelines"},{"location":"ocp4-upi-powervs/CODE_OF_CONDUCT/#1-correction","text":"Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.","title":"1. Correction"},{"location":"ocp4-upi-powervs/CODE_OF_CONDUCT/#2-warning","text":"Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.","title":"2. Warning"},{"location":"ocp4-upi-powervs/CODE_OF_CONDUCT/#3-temporary-ban","text":"Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.","title":"3. Temporary Ban"},{"location":"ocp4-upi-powervs/CODE_OF_CONDUCT/#4-permanent-ban","text":"Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community.","title":"4. Permanent Ban"},{"location":"ocp4-upi-powervs/CODE_OF_CONDUCT/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html. Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder .","title":"Attribution"},{"location":"ocp4-upi-powervs/CONTRIBUTING/","text":"Contributing This project is Apache 2.0 Licenced and welcomes external contributions. When contributing to this repository, please first discuss the change you wish to make via an issue . Please note we have a code of conduct , please follow it in all your interactions with the project. Issues If you find any issue with the code or documentation please submit an issue . It is best to check out existing issues first to see if a similar one is open or has already been discussed. Pull Request Process To contribute code or documentation, please submit a pull request . Always take the latest update from upstream/master before creating a pull request. Ensure your changes work fine and have no syntax problems. Also, verify that it does not break the existing code flow. Update the README.md or relevant documents with details of changes to the code. This includes variables change, added or updated feature, change in steps, dependencies change, etc. Make use of proper commit message. Mention the issue# which you are planning to address eg: Fixes #38. After creating the pull request ensure you implement all the review comments given if any. Pull request will be merged only when it has at least two approvals from the list of reviewers. Please read Developer Certificate of Origin and sign-off your commit using command git commit -s . Spec Formatting Conventions Documents in this repository will adhere to the following rules: Lines are wrapped at 80 columns (when possible) Use spaces to indent your code. Do not use tab character, instead can use 2/4 spaces.","title":"Contributing"},{"location":"ocp4-upi-powervs/CONTRIBUTING/#contributing","text":"This project is Apache 2.0 Licenced and welcomes external contributions. When contributing to this repository, please first discuss the change you wish to make via an issue . Please note we have a code of conduct , please follow it in all your interactions with the project.","title":"Contributing"},{"location":"ocp4-upi-powervs/CONTRIBUTING/#issues","text":"If you find any issue with the code or documentation please submit an issue . It is best to check out existing issues first to see if a similar one is open or has already been discussed.","title":"Issues"},{"location":"ocp4-upi-powervs/CONTRIBUTING/#pull-request-process","text":"To contribute code or documentation, please submit a pull request . Always take the latest update from upstream/master before creating a pull request. Ensure your changes work fine and have no syntax problems. Also, verify that it does not break the existing code flow. Update the README.md or relevant documents with details of changes to the code. This includes variables change, added or updated feature, change in steps, dependencies change, etc. Make use of proper commit message. Mention the issue# which you are planning to address eg: Fixes #38. After creating the pull request ensure you implement all the review comments given if any. Pull request will be merged only when it has at least two approvals from the list of reviewers. Please read Developer Certificate of Origin and sign-off your commit using command git commit -s .","title":"Pull Request Process"},{"location":"ocp4-upi-powervs/CONTRIBUTING/#spec-formatting-conventions","text":"Documents in this repository will adhere to the following rules: Lines are wrapped at 80 columns (when possible) Use spaces to indent your code. Do not use tab character, instead can use 2/4 spaces.","title":"Spec Formatting Conventions"},{"location":"ocp4-upi-powervs/docs/automation_host_prereqs/","text":"Automation Host Prerequisites Automation Host Prerequisites Configure Your Firewall Automation Host Setup Terraform PowerVS CLI Git Configure Your Firewall If your automation host is behind a firewall, you will need to ensure the following ports are open in order to use ssh, http, and https: - 22, 443, 80 These additional ports are required for the ocp cli ( oc ) post-install: - 6443 Automation Host Setup Install the following packages on the automation host. Select the appropriate install binaries based on your automation host platform - Mac/Linux/Windows. Terraform Terraform >= 0.13.0 : Please refer to the link for instructions on installing Terraform. For validating the version run terraform version command after install. PowerVS CLI PowerVS CLI : Please download and install the CLI by referring to the following instructions . Alternatively, you can use IBM Cloud shell directly from the browser itself. Git Git : Please refer to the link for instructions on installing Git.","title":"Automation Host Prerequisites"},{"location":"ocp4-upi-powervs/docs/automation_host_prereqs/#automation-host-prerequisites","text":"Automation Host Prerequisites Configure Your Firewall Automation Host Setup Terraform PowerVS CLI Git","title":"Automation Host Prerequisites"},{"location":"ocp4-upi-powervs/docs/automation_host_prereqs/#configure-your-firewall","text":"If your automation host is behind a firewall, you will need to ensure the following ports are open in order to use ssh, http, and https: - 22, 443, 80 These additional ports are required for the ocp cli ( oc ) post-install: - 6443","title":"Configure Your Firewall"},{"location":"ocp4-upi-powervs/docs/automation_host_prereqs/#automation-host-setup","text":"Install the following packages on the automation host. Select the appropriate install binaries based on your automation host platform - Mac/Linux/Windows.","title":"Automation Host Setup"},{"location":"ocp4-upi-powervs/docs/automation_host_prereqs/#terraform","text":"Terraform >= 0.13.0 : Please refer to the link for instructions on installing Terraform. For validating the version run terraform version command after install.","title":"Terraform"},{"location":"ocp4-upi-powervs/docs/automation_host_prereqs/#powervs-cli","text":"PowerVS CLI : Please download and install the CLI by referring to the following instructions . Alternatively, you can use IBM Cloud shell directly from the browser itself.","title":"PowerVS CLI"},{"location":"ocp4-upi-powervs/docs/automation_host_prereqs/#git","text":"Git : Please refer to the link for instructions on installing Git.","title":"Git"},{"location":"ocp4-upi-powervs/docs/known_issues/","text":"Known Issues This page lists the known issues and potential next steps when deploying OpenShift (OCP) in Power Systems Virtual Server (PowerVS) Terraform apply returns the following error Error : timeout - last error: Error connecting to bastion: dial tcp 161.156.139.82:22: connect: operation timed out Cause : The public network attached to bastion is not reachable. Ping to the public/external IP of bastion node (eg. 161.156.139.82) will not return any response Workaround : Re-run TF again and if it doesn't help, destroy the TF resources and re-run. If it doesn't work, then please open a support case with IBM Cloud to fix issue with reachability of public IP for PowerVS instance. RHCOS instances in dashboard shows \\\"Warning\\\" Status Cause : This is due to RSCT daemon not being available for RHCOS Workaround : None You can ignore this. This will be fixed soon. Terraform apply fails with instance is not reachable error Cause : Sometimes the instances don't boot and and accessing the instance console from IBM Cloud dashboard shows grub rescue Workaround : Rebooting the instance helps. If still facing the same issue, try destroying the TF resources and re-running the deployment Terraform apply fails with instance is not reachable error Cause : Sometimes the instances don't boot and and accessing the instance console from IBM Cloud dashboard shows connect: network is unreachable Cause : Unknown Workaround : Rebooting the instance helps. Terraform provisioning fails with the following error Error : Failed to get the instance Get https://eu-de.power-iaas.cloud.ibm.com/pcloud/v1/cloud-instances/d239797b-7b2e- 4790-a29d-439567556c83/pvm-instances/7d836f7d-8f21-4bef-9e10-ae6d8c2167a1: context deadline exceeded on modules/4_nodes/nodes.tf line 92, in resource \"ibm_pi_instance\" \"master\": 92: resource \"ibm_pi_instance\" \"master\" { Reapply will also fail **module.nodes.ibm_pi_instance.master[0]: Creating... Error: Failed to provision {\"description\":\"bad request: invalid name server name already exists for cloud- instance\",\"error\":\"bad request\"} on modules/4_nodes/nodes.tf line 92, in resource \"ibm_pi_instance\" \"master\": 92: resource \"ibm_pi_instance\" \"master\" { Cause : IBM Cloud API failed but resource got created and TF doesn't know that the resource got created. Workaround : Manually delete the resource from the dashboard and re-apply Terraform apply fails with these errors Error : module.install.null_resource.install (remote-exec): information. module.install.null_resource.install (remote-exec): changed: [192.168.25.12] => {\"ansible_facts\": {\"discovered_interpreter_python\": \"/usr/libexec/platform-python\"}, \"changed\": true, \"cmd\": \"if lsmod|grep -q 'ibmveth'; then\\n sudo sysctl -w net.ipv4.route.min_pmtu=1450;\\n sudo sysctl -w net.ipv4.ip_no_pmtu_disc=1;\\n echo 'net.ipv4.route.min_pmtu = 1450' | sudo tee --append /etc/sysctl.d/88-sysctl.conf > /dev/null;\\n echo 'net.ipv4.ip_no_pmtu_disc = 1' | sudo tee --append /etc/sysctl.d/88-sysctl.conf > /dev/null;\\nfi\\n\", \"delta\": \"0:00:00.078644\", \"end\": \"2020-09-18 16:25:46.414601\", \"rc\": 0, \"start\": \"2020-09-18 16:25:46.335957\", \"stderr\": \"\", \"stderr_lines\": [], \"stdout\": \"net.ipv4.route.min_pmtu = 1450\\nnet.ipv4.ip_no_pmtu_disc = 1\", \"stdout_lines\": [\"net.ipv4.route.min_pmtu = 1450\", \"net.ipv4.ip_no_pmtu_disc = 1\"]} > module.install.null_resource.install (remote-exec): NO MORE HOSTS LEFT ************************************************************* > module.install.null_resource.install (remote-exec): PLAY RECAP ********************************************************************* > module.install.null_resource.install (remote-exec): 192.168.25.105 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.12 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.121 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.15 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.235 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.39 : ok=1 changed=0 unreachable=1 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.5 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.82 : ok=28 changed=22 unreachable=0 failed=0 skipped=11 rescued=0 ignored=0 module.install.null_resource.config (remote-exec): TASK [Downloading OCP4 Installer] * * * * * * **** > module.install.null_resource.config: Still creating... [1m40s elapsed] > module.install.null_resource.config (remote-exec): fatal: [localhost]: FAILED! => {\"changed\": false, \"dest\": \"/usr/local/src/openshift-install-linux.tar.gz\", \"elapsed\": 10, \"msg\": \"Request failed: \", \"url\": \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp/4.5.4/openshift-install-linux.tar.gz\"} module.install.null_resource.install (remote-exec): fatal: [192.168.25.167]: UNREACHABLE! => {\"changed\": false, \"msg\": \"Failed to connect to the host via ssh: ssh: connect to host 192.168.25.167 port 22: Connection timed out\", \"unreachable\": true} > module.install.null_resource.install (remote-exec): NO MORE HOSTS LEFT ************************************************************* > module.install.null_resource.install (remote-exec): PLAY RECAP ********************************************************************* > module.install.null_resource.install (remote-exec): 192.168.25.107 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.165 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.167 : ok=1 changed=0 unreachable=1 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.212 : ok=28 changed=22 unreachable=0 failed=0 skipped=11 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.64 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.76 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.78 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.81 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Workaround : Re-run the terraform apply command.","title":"Known Issues"},{"location":"ocp4-upi-powervs/docs/known_issues/#known-issues","text":"This page lists the known issues and potential next steps when deploying OpenShift (OCP) in Power Systems Virtual Server (PowerVS)","title":"Known Issues"},{"location":"ocp4-upi-powervs/docs/known_issues/#terraform-apply-returns-the-following-error","text":"Error : timeout - last error: Error connecting to bastion: dial tcp 161.156.139.82:22: connect: operation timed out Cause : The public network attached to bastion is not reachable. Ping to the public/external IP of bastion node (eg. 161.156.139.82) will not return any response Workaround : Re-run TF again and if it doesn't help, destroy the TF resources and re-run. If it doesn't work, then please open a support case with IBM Cloud to fix issue with reachability of public IP for PowerVS instance.","title":"Terraform apply returns the following error"},{"location":"ocp4-upi-powervs/docs/known_issues/#rhcos-instances-in-dashboard-shows-warning-status","text":"Cause : This is due to RSCT daemon not being available for RHCOS Workaround : None You can ignore this. This will be fixed soon.","title":"RHCOS instances in dashboard shows \\\"Warning\\\" Status"},{"location":"ocp4-upi-powervs/docs/known_issues/#terraform-apply-fails-with-instance-is-not-reachable-error","text":"Cause : Sometimes the instances don't boot and and accessing the instance console from IBM Cloud dashboard shows grub rescue Workaround : Rebooting the instance helps. If still facing the same issue, try destroying the TF resources and re-running the deployment","title":"Terraform apply fails with instance is not reachable error"},{"location":"ocp4-upi-powervs/docs/known_issues/#terraform-apply-fails-with-instance-is-not-reachable-error_1","text":"Cause : Sometimes the instances don't boot and and accessing the instance console from IBM Cloud dashboard shows connect: network is unreachable Cause : Unknown Workaround : Rebooting the instance helps.","title":"Terraform apply fails with instance is not reachable error"},{"location":"ocp4-upi-powervs/docs/known_issues/#terraform-provisioning-fails-with-the-following-error","text":"Error : Failed to get the instance Get https://eu-de.power-iaas.cloud.ibm.com/pcloud/v1/cloud-instances/d239797b-7b2e- 4790-a29d-439567556c83/pvm-instances/7d836f7d-8f21-4bef-9e10-ae6d8c2167a1: context deadline exceeded on modules/4_nodes/nodes.tf line 92, in resource \"ibm_pi_instance\" \"master\": 92: resource \"ibm_pi_instance\" \"master\" { Reapply will also fail **module.nodes.ibm_pi_instance.master[0]: Creating... Error: Failed to provision {\"description\":\"bad request: invalid name server name already exists for cloud- instance\",\"error\":\"bad request\"} on modules/4_nodes/nodes.tf line 92, in resource \"ibm_pi_instance\" \"master\": 92: resource \"ibm_pi_instance\" \"master\" { Cause : IBM Cloud API failed but resource got created and TF doesn't know that the resource got created. Workaround : Manually delete the resource from the dashboard and re-apply","title":"Terraform provisioning fails with the following error"},{"location":"ocp4-upi-powervs/docs/known_issues/#terraform-apply-fails-with-these-errors","text":"Error : module.install.null_resource.install (remote-exec): information. module.install.null_resource.install (remote-exec): changed: [192.168.25.12] => {\"ansible_facts\": {\"discovered_interpreter_python\": \"/usr/libexec/platform-python\"}, \"changed\": true, \"cmd\": \"if lsmod|grep -q 'ibmveth'; then\\n sudo sysctl -w net.ipv4.route.min_pmtu=1450;\\n sudo sysctl -w net.ipv4.ip_no_pmtu_disc=1;\\n echo 'net.ipv4.route.min_pmtu = 1450' | sudo tee --append /etc/sysctl.d/88-sysctl.conf > /dev/null;\\n echo 'net.ipv4.ip_no_pmtu_disc = 1' | sudo tee --append /etc/sysctl.d/88-sysctl.conf > /dev/null;\\nfi\\n\", \"delta\": \"0:00:00.078644\", \"end\": \"2020-09-18 16:25:46.414601\", \"rc\": 0, \"start\": \"2020-09-18 16:25:46.335957\", \"stderr\": \"\", \"stderr_lines\": [], \"stdout\": \"net.ipv4.route.min_pmtu = 1450\\nnet.ipv4.ip_no_pmtu_disc = 1\", \"stdout_lines\": [\"net.ipv4.route.min_pmtu = 1450\", \"net.ipv4.ip_no_pmtu_disc = 1\"]} > module.install.null_resource.install (remote-exec): NO MORE HOSTS LEFT ************************************************************* > module.install.null_resource.install (remote-exec): PLAY RECAP ********************************************************************* > module.install.null_resource.install (remote-exec): 192.168.25.105 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.12 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.121 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.15 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.235 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.39 : ok=1 changed=0 unreachable=1 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.5 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.82 : ok=28 changed=22 unreachable=0 failed=0 skipped=11 rescued=0 ignored=0 module.install.null_resource.config (remote-exec): TASK [Downloading OCP4 Installer] * * * * * * **** > module.install.null_resource.config: Still creating... [1m40s elapsed] > module.install.null_resource.config (remote-exec): fatal: [localhost]: FAILED! => {\"changed\": false, \"dest\": \"/usr/local/src/openshift-install-linux.tar.gz\", \"elapsed\": 10, \"msg\": \"Request failed: \", \"url\": \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp/4.5.4/openshift-install-linux.tar.gz\"} module.install.null_resource.install (remote-exec): fatal: [192.168.25.167]: UNREACHABLE! => {\"changed\": false, \"msg\": \"Failed to connect to the host via ssh: ssh: connect to host 192.168.25.167 port 22: Connection timed out\", \"unreachable\": true} > module.install.null_resource.install (remote-exec): NO MORE HOSTS LEFT ************************************************************* > module.install.null_resource.install (remote-exec): PLAY RECAP ********************************************************************* > module.install.null_resource.install (remote-exec): 192.168.25.107 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.165 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.167 : ok=1 changed=0 unreachable=1 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.212 : ok=28 changed=22 unreachable=0 failed=0 skipped=11 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.64 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.76 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.78 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 > module.install.null_resource.install (remote-exec): 192.168.25.81 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Workaround : Re-run the terraform apply command.","title":"Terraform apply fails with these errors"},{"location":"ocp4-upi-powervs/docs/ocp_prereqs_powervs/","text":"PowerVS Prerequisites IBM Cloud Account You'll need to have an IBM Cloud Account to be able to use Power Systems Virtual Server (PowerVS). Create Power Systems Virtual Server Service Instance Login to IBM Cloud Dashboard and search for \" Power \" in the Catalog . Select \" Power Systems Virtual Server \" and provide all the required inputs to create the service instance. 1. Provide a meaningful name for your instance in the Service name field. 2. Select the proper resource group . More details on resource groups is available from the following link Create Private Network A private network is required for your OCP cluster. Choose the previously created \" Service Instance \" and create a private subnet by selecting \" Subnets \" and providing the required inputs. If you see a screen displaying CRN and GUID, then click \"View full details\" to access the \"Subnet\" creation page. You can create multiple OCP clusters in the same service instance using the same private network. If required you can also create multiple private networks. Provide the required inputs for private subnet creation Raise a Service Request to enable IP communication between PowerVS instances on private network In order for your instances to communicate within the subnet, you'll need to create a service request. Click on Support in the top bar and scroll down to Contact Support , then select \" Create a case \" Select \" Power Systems Virtual Server \" tile Complete the details as shown using the following template: [Subject:] Enable communication between PowerVS instances on private network [Body:] Please enable IP communication between PowerVS instances for the following private network: Name: <your-subnet-name-from-above> Type: Private CIDR: <your ip subnet-from-above> VLAN ID: <your-vlan-id> (listed in your subnet details post-creation) Location: <your-location> (listed in your subnet details post-creation) Service Instance: <your-service-name> Click \" Continue \" to accept agreements, and then Click \" Submit case \". RHCOS and RHEL 8.2 Images for OpenShift RHEL image is used for bastion and RHCOS is used for the OpenShift cluster nodes. You'll need to create OVA formatted images for RHEL and RHCOS, upload them to IBM Cloud Object storage and then import these images as boot images in your PowerVS service instance. Further, the image disk should be minimum of 120 GB in size. Creating OVA images If you have PowerVC then you can follow the instructions provided in the link to export an existing PowerVC image to OVA image. You can also use the following python script to convert Qcow2 image to OVA RHEL 8.2 Qcow2 image is available from the following link RHCOS Qcow2 image is available from the following link Uploading to IBM Cloud Object Storage Create IBM Cloud Object Storage service and bucket Please refer to the following link for instructions to create IBM Cloud Object Storage service and required storage bucket to upload the OVA images. Create secret and access keys with Hash-based Message Authentication Code (HMAC) Please refer to the following link for instructions to create the keys required for importing the images into your PowerVS service instance. Upload the OVA image to Cloud Object storage bucket Please refer to the following link for uploading the OVA image to the respective bucket. Alternatively you can also use the following python script . Importing the images in PowerVS Choose the previously created PowerVS \"Service Instance\", click \"View full details\" and select \"Boot images\". Click the \"Importing image\" option and fill the requisite details like image name, storage type and cloud object storage details. Example screenshot showing import of RHEL image that is used for bastion Example screenshot showing import of RHCOS image used for OCP Your PowerVS service instance is now ready for OpenShift clusters.","title":"**PowerVS Prerequisites**"},{"location":"ocp4-upi-powervs/docs/ocp_prereqs_powervs/#powervs-prerequisites","text":"","title":"PowerVS Prerequisites"},{"location":"ocp4-upi-powervs/docs/ocp_prereqs_powervs/#ibm-cloud-account","text":"You'll need to have an IBM Cloud Account to be able to use Power Systems Virtual Server (PowerVS).","title":"IBM Cloud Account"},{"location":"ocp4-upi-powervs/docs/ocp_prereqs_powervs/#create-power-systems-virtual-server-service-instance","text":"Login to IBM Cloud Dashboard and search for \" Power \" in the Catalog . Select \" Power Systems Virtual Server \" and provide all the required inputs to create the service instance. 1. Provide a meaningful name for your instance in the Service name field. 2. Select the proper resource group . More details on resource groups is available from the following link","title":"Create Power Systems Virtual Server Service Instance"},{"location":"ocp4-upi-powervs/docs/ocp_prereqs_powervs/#create-private-network","text":"A private network is required for your OCP cluster. Choose the previously created \" Service Instance \" and create a private subnet by selecting \" Subnets \" and providing the required inputs. If you see a screen displaying CRN and GUID, then click \"View full details\" to access the \"Subnet\" creation page. You can create multiple OCP clusters in the same service instance using the same private network. If required you can also create multiple private networks. Provide the required inputs for private subnet creation","title":"Create Private Network"},{"location":"ocp4-upi-powervs/docs/ocp_prereqs_powervs/#raise-a-service-request-to-enable-ip-communication-between-powervs-instances-on-private-network","text":"In order for your instances to communicate within the subnet, you'll need to create a service request. Click on Support in the top bar and scroll down to Contact Support , then select \" Create a case \" Select \" Power Systems Virtual Server \" tile Complete the details as shown using the following template: [Subject:] Enable communication between PowerVS instances on private network [Body:] Please enable IP communication between PowerVS instances for the following private network: Name: <your-subnet-name-from-above> Type: Private CIDR: <your ip subnet-from-above> VLAN ID: <your-vlan-id> (listed in your subnet details post-creation) Location: <your-location> (listed in your subnet details post-creation) Service Instance: <your-service-name> Click \" Continue \" to accept agreements, and then Click \" Submit case \".","title":"Raise a Service Request to enable IP communication between PowerVS instances on private network"},{"location":"ocp4-upi-powervs/docs/ocp_prereqs_powervs/#rhcos-and-rhel-82-images-for-openshift","text":"RHEL image is used for bastion and RHCOS is used for the OpenShift cluster nodes. You'll need to create OVA formatted images for RHEL and RHCOS, upload them to IBM Cloud Object storage and then import these images as boot images in your PowerVS service instance. Further, the image disk should be minimum of 120 GB in size.","title":"RHCOS and RHEL 8.2 Images for OpenShift"},{"location":"ocp4-upi-powervs/docs/ocp_prereqs_powervs/#creating-ova-images","text":"If you have PowerVC then you can follow the instructions provided in the link to export an existing PowerVC image to OVA image. You can also use the following python script to convert Qcow2 image to OVA RHEL 8.2 Qcow2 image is available from the following link RHCOS Qcow2 image is available from the following link","title":"Creating OVA images"},{"location":"ocp4-upi-powervs/docs/ocp_prereqs_powervs/#uploading-to-ibm-cloud-object-storage","text":"Create IBM Cloud Object Storage service and bucket Please refer to the following link for instructions to create IBM Cloud Object Storage service and required storage bucket to upload the OVA images. Create secret and access keys with Hash-based Message Authentication Code (HMAC) Please refer to the following link for instructions to create the keys required for importing the images into your PowerVS service instance. Upload the OVA image to Cloud Object storage bucket Please refer to the following link for uploading the OVA image to the respective bucket. Alternatively you can also use the following python script .","title":"Uploading to IBM Cloud Object Storage"},{"location":"ocp4-upi-powervs/docs/ocp_prereqs_powervs/#importing-the-images-in-powervs","text":"Choose the previously created PowerVS \"Service Instance\", click \"View full details\" and select \"Boot images\". Click the \"Importing image\" option and fill the requisite details like image name, storage type and cloud object storage details. Example screenshot showing import of RHEL image that is used for bastion Example screenshot showing import of RHCOS image used for OCP Your PowerVS service instance is now ready for OpenShift clusters.","title":"Importing the images in PowerVS"},{"location":"ocp4-upi-powervs/docs/quickstart/","text":"Installation Quickstart Installation Quickstart Download the Automation Code Setup Terraform Variables Start Install Post Install Delete Bootstrap Node Create API and Ingress DNS Records Cluster Access Using CLI Using Web UI Clean up Download the Automation Code You'll need to use git to clone the deployment code when working off the master branch $ git clone https://github.com/ocp-power-automation/ocp4-upi-powervs.git $ cd ocp4-upi-powervs All further instructions assumes you are in the code directory eg. ocp4-upi-powervs Setup Terraform Variables Update the var.tfvars based on your environment. Description of the variables are available in the following link . You can use environment variables for sensitive data that should not be saved to disk. $ set +o history $ export IBMCLOUD_API_KEY=xxxxxxxxxxxxxxx $ export RHEL_SUBS_USERNAME=xxxxxxxxxxxxxxx $ export RHEL_SUBS_PASSWORD=xxxxxxxxxxxxxxx $ set -o history Start Install Run the following commands from within the directory. $ terraform init $ terraform apply -var-file var.tfvars -parallelism=3 If using environment variables for sensitive data, then do the following, instead. $ terraform init $ terraform apply -var-file var.tfvars -parallelism=3 -var ibmcloud_api_key=\"$IBMCLOUD_API_KEY\" -var rhel_subscription_username=\"$RHEL_SUBS_USERNAME\" -var rhel_subscription_password=\"$RHEL_SUBS_PASSWORD\" Note : We have used parallelism to restrict parallel instance creation requests using the PowerVS client. This is due to a known issue where the apply fails at random parallel instance create requests. If you still get the error while creating the instance, you will have to delete the failed instance from PowerVS console and then run the apply command again. Now wait for the installation to complete. It may take around 60 mins to complete provisioning. On successful install cluster details will be printed as shown below. bastion_private_ip = 192.168.25.171 bastion_public_ip = 16.20.34.5 bastion_ssh_command = ssh -i data/id_rsa root@16.20.34.5 bootstrap_ip = 192.168.25.182 cluster_authentication_details = Cluster authentication details are available in 16.20.34.5 under ~/openstack-upi/auth cluster_id = test-cluster-9a4f etc_hosts_entries = 16.20.34.5 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com install_status = COMPLETED master_ips = [ \"192.168.25.147\", \"192.168.25.176\", ] oc_server_url = https://test-cluster-9a4f.mydomain.com:6443 storageclass_name = nfs-storage-provisioner web_console_url = https://console-openshift-console.apps.test-cluster-9a4f.mydomain.com worker_ips = [ \"192.168.25.220\", \"192.168.25.134\", ] When using wildcard domain like nip.io or xip.io then etc_host_entries is empty bastion_private_ip = 192.168.25.171 bastion_public_ip = 16.20.34.5 bastion_ssh_command = ssh -i data/id_rsa root@16.20.34.5 bootstrap_ip = 192.168.25.182 cluster_authentication_details = Cluster authentication details are available in 16.20.34.5 under ~/openstack-upi/auth cluster_id = test-cluster-9a4f etc_hosts_entries = install_status = COMPLETED master_ips = [ \"192.168.25.147\", \"192.168.25.176\", ] oc_server_url = https://test-cluster-9a4f.16.20.34.5.nip.io:6443 storageclass_name = nfs-storage-provisioner web_console_url = https://console-openshift-console.apps.test-cluster-9a4f.16.20.34.5.nip.io worker_ips = [ \"192.168.25.220\", \"192.168.25.134\", ] These details can be retrieved anytime by running the following command from the root folder of the code $ terraform output In case of any errors, you'll have to re-apply. Please refer to known issues to get more details on potential issues and workarounds. Post Install Delete Bootstrap Node Once the deployment is completed successfully, you can safely delete the bootstrap node. This step is optional but recommended so as to free up the resources used. Change the count value to 0 in bootstrap map variable and re-run the apply command. Eg: bootstrap = {memory = \"16\", processors = \"0.5\", \"count\" = 0} Run command terraform apply -var-file var.tfvars Create API and Ingress DNS Records Please skip this section if your cluster_domain is one of the online wildcard DNS domains: nip.io, xip.io and sslip.io. For all other domains, you can use one of the following options. Add entries to your DNS server The general format is shown below: api.<cluster_id>. IN A <bastion_public_ip> *.apps.<cluster_id>. IN A <bastion_public_ip> You'll need bastion_public_ip and cluster_id . This is printed at the end of a successful install. Or you can retrieve it anytime by running terraform output from the install directory. For example, if bastion_public_ip = 16.20.34.5 and cluster_id = test-cluster-9a4f then the following DNS records will need to be added. api.test-cluster-9a4f. IN A 16.20.34.5 *.apps.test-cluster-9a4f. IN A 16.20.34.5 Add entries to your client system hosts file For Linux and Mac hosts file is located at /etc/hosts and for Windows it's located at c:\\Windows\\System32\\Drivers\\etc\\hosts . The general format is shown below: <bastion_public_ip> api.<cluster_id> <bastion_public_ip> console-openshift-console.apps.<cluster_id> <bastion_public_ip> integrated-oauth-server-openshift-authentication.apps.<cluster_id> <bastion_public_ip> oauth-openshift.apps.<cluster_id> <bastion_public_ip> prometheus-k8s-openshift-monitoring.apps.<cluster_id> <bastion_public_ip> grafana-openshift-monitoring.apps.<cluster_id> <bastion_public_ip> <app name>.apps.<cluster_id> You'll need etc_host_entries . This is printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. As an example, for the following etc_hosts_entries etc_hosts_entries = 16.20.34.5 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com just add the following entry to the hosts file ``` [existing entries in hosts file] 16.20.34.5 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com ``` Cluster Access OpenShift login credentials are in the bastion host and the location will be printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. [...] bastion_public_ip = 16.20.34.5 bastion_ssh_command = ssh -i data/id_rsa root@16.20.34.5 cluster_authentication_details = Cluster authentication details are available in 16.20.34.5 under ~/openstack-upi/auth [...] There are two files under ~/openstack-upi/auth - kubeconfig : can be used for CLI access - kubeadmin-password : Password for kubeadmin user which can be used for CLI, UI access Note : Ensure you securely store the OpenShift cluster access credentials. If desired delete the access details from the bastion node after securely storing the same. You can copy the access details to your local system $ scp -r -i data/id_rsa root@158.175.161.118:~/openstack-upi/auth/\\* . Using CLI OpenShift CLI oc can be downloaded from the following links. Use the one specific to your client system architecture. Mac OSX Linux (x86_64) Windows Download the specific file, extract it and place the binary in a directory that is on your PATH For more details check the following link The CLI login URL oc_server_url will be printed at the end of successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. [...] oc_server_url = https://test-cluster-9a4f.mydomain.com:6443 [...] In order to login the cluster you can use the oc login <oc_server_url> -u kubeadmin -p <kubeadmin-password> Example: $ oc login https://test-cluster-9a4f.mydomain.com:6443 -u kubeadmin -p $(cat kubeadmin-password) You can also use the kubeconfig file $ export KUBECONFIG=$(pwd)/kubeconfig $ oc cluster-info Kubernetes master is running at https://test-cluster-9a4f.mydomain.com:6443 To further debug and diagnose cluster problems, use 'kubectl cluster-info dump' $ oc get nodes NAME STATUS ROLES AGE VERSION master-0 Ready master 13h v1.18.3+b74c5ed master-1 Ready master 13h v1.18.3+b74c5ed master-2 Ready master 13h v1.18.3+b74c5ed worker-0 Ready worker 13h v1.18.3+b74c5ed worker-1 Ready worker 13h v1.18.3+b74c5ed Note: The OpenShift command-line client oc is already configured on the bastion node with kubeconfig placed at ~/.kube/config . Using Web UI The web console URL will be printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. [...] web_console_url = https://console-openshift-console.apps.test-cluster-9a4f.mydomain.com [...] Open this URL in your browser and login with user kubeadmin and password mentioned in the kubeadmin-password file. Clean up To destroy after you are done using the cluster you can run command terraform destroy -var-file var.tfvars -parallelism=3 to make sure that all resources are properly cleaned up. Do not manually clean up your environment unless both of the following are true: You know what you are doing Something went wrong with an automated deletion.","title":"Installation Quickstart"},{"location":"ocp4-upi-powervs/docs/quickstart/#installation-quickstart","text":"Installation Quickstart Download the Automation Code Setup Terraform Variables Start Install Post Install Delete Bootstrap Node Create API and Ingress DNS Records Cluster Access Using CLI Using Web UI Clean up","title":"Installation Quickstart"},{"location":"ocp4-upi-powervs/docs/quickstart/#download-the-automation-code","text":"You'll need to use git to clone the deployment code when working off the master branch $ git clone https://github.com/ocp-power-automation/ocp4-upi-powervs.git $ cd ocp4-upi-powervs All further instructions assumes you are in the code directory eg. ocp4-upi-powervs","title":"Download the Automation Code"},{"location":"ocp4-upi-powervs/docs/quickstart/#setup-terraform-variables","text":"Update the var.tfvars based on your environment. Description of the variables are available in the following link . You can use environment variables for sensitive data that should not be saved to disk. $ set +o history $ export IBMCLOUD_API_KEY=xxxxxxxxxxxxxxx $ export RHEL_SUBS_USERNAME=xxxxxxxxxxxxxxx $ export RHEL_SUBS_PASSWORD=xxxxxxxxxxxxxxx $ set -o history","title":"Setup Terraform Variables"},{"location":"ocp4-upi-powervs/docs/quickstart/#start-install","text":"Run the following commands from within the directory. $ terraform init $ terraform apply -var-file var.tfvars -parallelism=3 If using environment variables for sensitive data, then do the following, instead. $ terraform init $ terraform apply -var-file var.tfvars -parallelism=3 -var ibmcloud_api_key=\"$IBMCLOUD_API_KEY\" -var rhel_subscription_username=\"$RHEL_SUBS_USERNAME\" -var rhel_subscription_password=\"$RHEL_SUBS_PASSWORD\" Note : We have used parallelism to restrict parallel instance creation requests using the PowerVS client. This is due to a known issue where the apply fails at random parallel instance create requests. If you still get the error while creating the instance, you will have to delete the failed instance from PowerVS console and then run the apply command again. Now wait for the installation to complete. It may take around 60 mins to complete provisioning. On successful install cluster details will be printed as shown below. bastion_private_ip = 192.168.25.171 bastion_public_ip = 16.20.34.5 bastion_ssh_command = ssh -i data/id_rsa root@16.20.34.5 bootstrap_ip = 192.168.25.182 cluster_authentication_details = Cluster authentication details are available in 16.20.34.5 under ~/openstack-upi/auth cluster_id = test-cluster-9a4f etc_hosts_entries = 16.20.34.5 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com install_status = COMPLETED master_ips = [ \"192.168.25.147\", \"192.168.25.176\", ] oc_server_url = https://test-cluster-9a4f.mydomain.com:6443 storageclass_name = nfs-storage-provisioner web_console_url = https://console-openshift-console.apps.test-cluster-9a4f.mydomain.com worker_ips = [ \"192.168.25.220\", \"192.168.25.134\", ] When using wildcard domain like nip.io or xip.io then etc_host_entries is empty bastion_private_ip = 192.168.25.171 bastion_public_ip = 16.20.34.5 bastion_ssh_command = ssh -i data/id_rsa root@16.20.34.5 bootstrap_ip = 192.168.25.182 cluster_authentication_details = Cluster authentication details are available in 16.20.34.5 under ~/openstack-upi/auth cluster_id = test-cluster-9a4f etc_hosts_entries = install_status = COMPLETED master_ips = [ \"192.168.25.147\", \"192.168.25.176\", ] oc_server_url = https://test-cluster-9a4f.16.20.34.5.nip.io:6443 storageclass_name = nfs-storage-provisioner web_console_url = https://console-openshift-console.apps.test-cluster-9a4f.16.20.34.5.nip.io worker_ips = [ \"192.168.25.220\", \"192.168.25.134\", ] These details can be retrieved anytime by running the following command from the root folder of the code $ terraform output In case of any errors, you'll have to re-apply. Please refer to known issues to get more details on potential issues and workarounds.","title":"Start Install"},{"location":"ocp4-upi-powervs/docs/quickstart/#post-install","text":"","title":"Post Install"},{"location":"ocp4-upi-powervs/docs/quickstart/#delete-bootstrap-node","text":"Once the deployment is completed successfully, you can safely delete the bootstrap node. This step is optional but recommended so as to free up the resources used. Change the count value to 0 in bootstrap map variable and re-run the apply command. Eg: bootstrap = {memory = \"16\", processors = \"0.5\", \"count\" = 0} Run command terraform apply -var-file var.tfvars","title":"Delete Bootstrap Node"},{"location":"ocp4-upi-powervs/docs/quickstart/#create-api-and-ingress-dns-records","text":"Please skip this section if your cluster_domain is one of the online wildcard DNS domains: nip.io, xip.io and sslip.io. For all other domains, you can use one of the following options. Add entries to your DNS server The general format is shown below: api.<cluster_id>. IN A <bastion_public_ip> *.apps.<cluster_id>. IN A <bastion_public_ip> You'll need bastion_public_ip and cluster_id . This is printed at the end of a successful install. Or you can retrieve it anytime by running terraform output from the install directory. For example, if bastion_public_ip = 16.20.34.5 and cluster_id = test-cluster-9a4f then the following DNS records will need to be added. api.test-cluster-9a4f. IN A 16.20.34.5 *.apps.test-cluster-9a4f. IN A 16.20.34.5 Add entries to your client system hosts file For Linux and Mac hosts file is located at /etc/hosts and for Windows it's located at c:\\Windows\\System32\\Drivers\\etc\\hosts . The general format is shown below: <bastion_public_ip> api.<cluster_id> <bastion_public_ip> console-openshift-console.apps.<cluster_id> <bastion_public_ip> integrated-oauth-server-openshift-authentication.apps.<cluster_id> <bastion_public_ip> oauth-openshift.apps.<cluster_id> <bastion_public_ip> prometheus-k8s-openshift-monitoring.apps.<cluster_id> <bastion_public_ip> grafana-openshift-monitoring.apps.<cluster_id> <bastion_public_ip> <app name>.apps.<cluster_id> You'll need etc_host_entries . This is printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. As an example, for the following etc_hosts_entries etc_hosts_entries = 16.20.34.5 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com just add the following entry to the hosts file ``` [existing entries in hosts file] 16.20.34.5 api.test-cluster-9a4f.mydomain.com console-openshift-console.apps.test-cluster-9a4f.mydomain.com integrated-oauth-server-openshift-authentication.apps.test-cluster-9a4f.mydomain.com oauth-openshift.apps.test-cluster-9a4f.mydomain.com prometheus-k8s-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com grafana-openshift-monitoring.apps.test-cluster-9a4f.mydomain.com example.apps.test-cluster-9a4f.mydomain.com ```","title":"Create API and Ingress DNS Records"},{"location":"ocp4-upi-powervs/docs/quickstart/#cluster-access","text":"OpenShift login credentials are in the bastion host and the location will be printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. [...] bastion_public_ip = 16.20.34.5 bastion_ssh_command = ssh -i data/id_rsa root@16.20.34.5 cluster_authentication_details = Cluster authentication details are available in 16.20.34.5 under ~/openstack-upi/auth [...] There are two files under ~/openstack-upi/auth - kubeconfig : can be used for CLI access - kubeadmin-password : Password for kubeadmin user which can be used for CLI, UI access Note : Ensure you securely store the OpenShift cluster access credentials. If desired delete the access details from the bastion node after securely storing the same. You can copy the access details to your local system $ scp -r -i data/id_rsa root@158.175.161.118:~/openstack-upi/auth/\\* .","title":"Cluster Access"},{"location":"ocp4-upi-powervs/docs/quickstart/#using-cli","text":"OpenShift CLI oc can be downloaded from the following links. Use the one specific to your client system architecture. Mac OSX Linux (x86_64) Windows Download the specific file, extract it and place the binary in a directory that is on your PATH For more details check the following link The CLI login URL oc_server_url will be printed at the end of successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. [...] oc_server_url = https://test-cluster-9a4f.mydomain.com:6443 [...] In order to login the cluster you can use the oc login <oc_server_url> -u kubeadmin -p <kubeadmin-password> Example: $ oc login https://test-cluster-9a4f.mydomain.com:6443 -u kubeadmin -p $(cat kubeadmin-password) You can also use the kubeconfig file $ export KUBECONFIG=$(pwd)/kubeconfig $ oc cluster-info Kubernetes master is running at https://test-cluster-9a4f.mydomain.com:6443 To further debug and diagnose cluster problems, use 'kubectl cluster-info dump' $ oc get nodes NAME STATUS ROLES AGE VERSION master-0 Ready master 13h v1.18.3+b74c5ed master-1 Ready master 13h v1.18.3+b74c5ed master-2 Ready master 13h v1.18.3+b74c5ed worker-0 Ready worker 13h v1.18.3+b74c5ed worker-1 Ready worker 13h v1.18.3+b74c5ed Note: The OpenShift command-line client oc is already configured on the bastion node with kubeconfig placed at ~/.kube/config .","title":"Using CLI"},{"location":"ocp4-upi-powervs/docs/quickstart/#using-web-ui","text":"The web console URL will be printed at the end of a successful install. Alternatively you can retrieve it anytime by running terraform output from the install directory. [...] web_console_url = https://console-openshift-console.apps.test-cluster-9a4f.mydomain.com [...] Open this URL in your browser and login with user kubeadmin and password mentioned in the kubeadmin-password file.","title":"Using Web UI"},{"location":"ocp4-upi-powervs/docs/quickstart/#clean-up","text":"To destroy after you are done using the cluster you can run command terraform destroy -var-file var.tfvars -parallelism=3 to make sure that all resources are properly cleaned up. Do not manually clean up your environment unless both of the following are true: You know what you are doing Something went wrong with an automated deletion.","title":"Clean up"},{"location":"ocp4-upi-powervs/docs/var.tfvars-doc/","text":"How to use var.tfvars How to use var.tfvars Introduction IBM Cloud Details OpenShift Cluster Details OpenShift Installation Details Misc Customizations Introduction This guide gives an overview of the various terraform variables that are used for the deployment. The default values are set in variables.tf IBM Cloud Details These set of variables specify the access key and PowerVS location details. ibmcloud_api_key = \"xyzaaaaaaaabcdeaaaaaa\" ibmcloud_region = \"xya\" ibmcloud_zone = \"abc\" service_instance_id = \"abc123xyzaaaa\" You'll need to create an API key to use the automation code. Please refer to the following instructions to generate API key - https://cloud.ibm.com/docs/account?topic=account-userapikey In order to retrieve the PowerVS region, zone and instance specific details please use the IBM Cloud CLI. Run ibmcloud pi service-list . It will list the service instance names with IDs. The ID will be of the form crn:v1:bluemix:public:power-iaas:eu-de-1:a/65b64c1f1c29460e8c2e4bbfbd893c2c:360a5df8-3f00-44b2-bd9f-d9a51fe53de6:: The 6th field is the ibmcloud_zone and 8th field is the service_instance_id $ echo \"crn:v1:bluemix:public:power-iaas:eu-de-1:a/65b64c1f1c29460e8c2e4bbfbd893c2c:360a5df8-3f00-44b2-bd9f-d9a51fe53de6::\" | cut -f6,8 -d\":\" eu-de-1:360a5df8-3f00-44b2-bd9f-d9a51fe53de6 Following are the region and zone mapping: ibmcloud_region ibmcloud_zone eu-de eu-de-1 lon lon0 tor tor01 Tieing all these, the values to be used will be as shown below: ibmcloud_region = eu-de ibmcloud_zone = eu-de-1 service_instance_id = 360a5df8-3f00-44b2-bd9f-d9a51fe53de6 OpenShift Cluster Details These set of variables specify the cluster capacity. Change the values as per your requirement. The defaults (recommended config) should suffice for most of the common use-cases. bastion = {memory = \"16\", processors = \"1\", \"count\" = 1} bootstrap = {memory = \"16\", processors = \"0.5\", \"count\" = 1} master = {memory = \"16\", processors = \"0.5\", \"count\" = 3} worker = {memory = \"32\", processors = \"0.5\", \"count\" = 2} memory is in GBs and count specifies the number of VMs that should be created for each type. To enable high availability (HA) for cluster services running on the bastion set the bastion count value to 2. Note that in case of HA, the automation will not setup NFS storage. count of 1 for bastion implies the default non-HA bastion setup. You can optionally set the worker count value to 0 in which case all the cluster pods will be running on the master/supervisor nodes. Ensure you use proper sizing for master/supervisor nodes to avoid resource starvation for containers. For PowerVS processors are equal to entitled physical count. So N processors == N physical core entitlements == ceil[N] vCPUs. Here are some examples to help you understand the relationship. Example 1 0.5 processors == 0.5 physical core entitlements == ceil[0.5] = 1 vCPU == 8 logical OS CPUs (SMT=8) Example 2 1.5 processors == 1.5 physical core entitlements == ceil[1.5] = 2 vCPU == 16 logical OS CPUs (SMT=8) Example 3 2 processors == 2 physical core entitlements == ceil[2] = 2 vCPU == 16 logical OS CPUs (SMT=8) These set of variables specify the RHEL and RHCOS boot image names. These images should have been already imported in your PowerVS service instance. Change the image names according to your environment. Ensure that you use the correct RHCOS image specific to the pre-release version rhel_image_name = \"<rhel_or_centos_image-name>\" rhcos_image_name = \"<rhcos-image-name>\" Note that the boot images should have a minimum disk size of 120GB This variable specifies the name of the private network that is configured in your PowerVS service instance. network_name = \"ocp-net\" These set of variables specify the type of processor and physical system type to be used for the VMs. Change the default values according to your requirement. processor_type = \"shared\" #Can be shared or dedicated system_type = \"s922\" #Can be either s922 or e980 These set of variables specify the username and the SSH key to be used for accessing the bastion node. rhel_username = \"root\" public_key_file = \"data/id_rsa.pub\" private_key_file = \"data/id_rsa\" Please note that only OpenSSH formatted keys are supported. Refer to the following links for instructions on creating SSH key based on your platform. - Windows 10 - https://phoenixnap.com/kb/generate-ssh-key-windows-10 - Mac OSX - https://www.techrepublic.com/article/how-to-generate-ssh-keys-on-macos-mojave/ - Linux - https://www.siteground.com/kb/generate_ssh_key_in_linux/ Create the SSH key-pair and keep it under the data directory These set of variables specify the RHEL subscription details. This is sensitive data, and if you don't want to save it on disk, use environment variables RHEL_SUBS_USERNAME and RHEL_SUBS_PASSWORD and pass them to terraform apply command as shown in the Quickstart guide . If you are using CentOS as the bastion image, then leave these variables as-is. rhel_subscription_username = \"user@test.com\" rhel_subscription_password = \"mypassword\" This variable specifies the number of hardware threads (SMT) that's used for the bastion node. Default setting should be fine for majority of the use-cases. rhel_smt = 4 OpenShift Installation Details These variables specify the URL for the OpenShift installer and client binaries. Change the URL to the specific pre-release version that you want to install on PowerVS. Reference link - https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview openshift_install_tarball = \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview/latest/openshift-install-linux.tar.gz\" openshift_client_tarball = \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview/latest/openshift-client-linux.tar.gz\" This variable specifies the OpenShift pull secret. This is available from the following link - https://cloud.redhat.com/openshift/install/power/user-provisioned Download the secret and copy it to data/pull-secret.txt . pull_secret_file = \"data/pull-secret.txt\" These variables specifies the OpenShift cluster domain details. Edit it as per your requirements. cluster_domain = \"ibm.com\" cluster_id_prefix = \"test-ocp\" cluster_id = \"\" Set the cluster_domain to nip.io , xip.io or sslip.io if you prefer using online wildcard domains. Default is ibm.com . The cluster_id_prefix should not be more than 8 characters. Nodes are pre-fixed with this value. Default value is test-ocp A random value will be used for cluster_id if not set. The total length of cluster_id_prefix . cluster_id should not exceed 14 characters. Misc Customizations These variables provides miscellaneous customizations. For common usage scenarios these are not required and should be left unchanged. The following variables can be used for disconnected install by using a local mirror registry on the bastion node. enable_local_registry = false #Set to true to enable usage of local registry for restricted network install. local_registry_image = \"docker.io/ibmcom/registry-ppc64le:2.6.2.5\" ocp_release_tag = \"4.4.9-ppc64le\" ocp_release_name = \"ocp-release\" This variable can be used for trying out custom OpenShift install image for development use. release_image_override = \"\" These variables specify the ansible playbooks that are used for OpenShift install and post-install customizations. helpernode_repo = \"https://github.com/RedHatOfficial/ocp4-helpernode\" helpernode_tag = \"5eab3db53976bb16be582f2edc2de02f7510050d\" install_playbook_repo = \"https://github.com/ocp-power-automation/ocp4-playbooks\" install_playbook_tag = \"02a598faa332aa2c3d53e8edd0e840440ff74bd5\" These variables can be used when debugging ansible playbooks installer_log_level = \"info\" ansible_extra_options = \"-v\" This variable specifies the external DNS servers to forward DNS queries that cannot be resolved locally. dns_forwarders = \"1.1.1.1; 9.9.9.9\" List of kernel arguments for the cluster nodes. Note that this will be applied after the cluster is installed and all the nodes are in Ready status. rhcos_kernel_options = [] Example 1 rhcos_kernel_options = [\"slub_max_order=0\",\"loglevel=7\"] These are NTP specific variables that are used for time-synchronization in the OpenShift cluster. chrony_config = true chrony_config_servers = [ {server = \"0.centos.pool.ntp.org\", options = \"iburst\"}, {server = \"1.centos.pool.ntp.org\", options = \"iburst\"} ] These set of variables are specific for cluster wide proxy configuration. Public internet access for the OpenShift cluster nodes is via Squid proxy deployed on the bastion. setup_squid_proxy = true If you have a separate proxy, and don't want to set the Squid proxy on bastion then use the following variables. setup_squid_proxy = false proxy = {server = \"hostname_or_ip\", port = \"3128\", user = \"pxuser\", password = \"pxpassword\"} Except server all other attributes are optional. Default port is 3128 with unauthenticated access. These variables specify details about NFS storage that is setup by default on the bastion server. storage_type = \"nfs\" volume_size = \"300\" # Value in GB volume_type = \"tier3\" volume_shareable = false If you need to attach additional data volumes to the OpenShift cluster nodes use the following variables. master_volume_size = \"500\" worker_volume_size = \"500\" The following variables are specific to upgrading an existing installation. upgrade_image = \"\" upgrade_pause_time = \"90\" upgrade_delay_time = \"600\" The following variables are specific to enable the connectivity between OCP nodes in PowerVS and IBM Cloud infrastructure over DirectLink. ibm_cloud_dl_endpoint_net_cidr = \"\" ibm_cloud_http_proxy = \"\"","title":"How to use var.tfvars"},{"location":"ocp4-upi-powervs/docs/var.tfvars-doc/#how-to-use-vartfvars","text":"How to use var.tfvars Introduction IBM Cloud Details OpenShift Cluster Details OpenShift Installation Details Misc Customizations","title":"How to use var.tfvars"},{"location":"ocp4-upi-powervs/docs/var.tfvars-doc/#introduction","text":"This guide gives an overview of the various terraform variables that are used for the deployment. The default values are set in variables.tf","title":"Introduction"},{"location":"ocp4-upi-powervs/docs/var.tfvars-doc/#ibm-cloud-details","text":"These set of variables specify the access key and PowerVS location details. ibmcloud_api_key = \"xyzaaaaaaaabcdeaaaaaa\" ibmcloud_region = \"xya\" ibmcloud_zone = \"abc\" service_instance_id = \"abc123xyzaaaa\" You'll need to create an API key to use the automation code. Please refer to the following instructions to generate API key - https://cloud.ibm.com/docs/account?topic=account-userapikey In order to retrieve the PowerVS region, zone and instance specific details please use the IBM Cloud CLI. Run ibmcloud pi service-list . It will list the service instance names with IDs. The ID will be of the form crn:v1:bluemix:public:power-iaas:eu-de-1:a/65b64c1f1c29460e8c2e4bbfbd893c2c:360a5df8-3f00-44b2-bd9f-d9a51fe53de6:: The 6th field is the ibmcloud_zone and 8th field is the service_instance_id $ echo \"crn:v1:bluemix:public:power-iaas:eu-de-1:a/65b64c1f1c29460e8c2e4bbfbd893c2c:360a5df8-3f00-44b2-bd9f-d9a51fe53de6::\" | cut -f6,8 -d\":\" eu-de-1:360a5df8-3f00-44b2-bd9f-d9a51fe53de6 Following are the region and zone mapping: ibmcloud_region ibmcloud_zone eu-de eu-de-1 lon lon0 tor tor01 Tieing all these, the values to be used will be as shown below: ibmcloud_region = eu-de ibmcloud_zone = eu-de-1 service_instance_id = 360a5df8-3f00-44b2-bd9f-d9a51fe53de6","title":"IBM Cloud Details"},{"location":"ocp4-upi-powervs/docs/var.tfvars-doc/#openshift-cluster-details","text":"These set of variables specify the cluster capacity. Change the values as per your requirement. The defaults (recommended config) should suffice for most of the common use-cases. bastion = {memory = \"16\", processors = \"1\", \"count\" = 1} bootstrap = {memory = \"16\", processors = \"0.5\", \"count\" = 1} master = {memory = \"16\", processors = \"0.5\", \"count\" = 3} worker = {memory = \"32\", processors = \"0.5\", \"count\" = 2} memory is in GBs and count specifies the number of VMs that should be created for each type. To enable high availability (HA) for cluster services running on the bastion set the bastion count value to 2. Note that in case of HA, the automation will not setup NFS storage. count of 1 for bastion implies the default non-HA bastion setup. You can optionally set the worker count value to 0 in which case all the cluster pods will be running on the master/supervisor nodes. Ensure you use proper sizing for master/supervisor nodes to avoid resource starvation for containers. For PowerVS processors are equal to entitled physical count. So N processors == N physical core entitlements == ceil[N] vCPUs. Here are some examples to help you understand the relationship. Example 1 0.5 processors == 0.5 physical core entitlements == ceil[0.5] = 1 vCPU == 8 logical OS CPUs (SMT=8) Example 2 1.5 processors == 1.5 physical core entitlements == ceil[1.5] = 2 vCPU == 16 logical OS CPUs (SMT=8) Example 3 2 processors == 2 physical core entitlements == ceil[2] = 2 vCPU == 16 logical OS CPUs (SMT=8) These set of variables specify the RHEL and RHCOS boot image names. These images should have been already imported in your PowerVS service instance. Change the image names according to your environment. Ensure that you use the correct RHCOS image specific to the pre-release version rhel_image_name = \"<rhel_or_centos_image-name>\" rhcos_image_name = \"<rhcos-image-name>\" Note that the boot images should have a minimum disk size of 120GB This variable specifies the name of the private network that is configured in your PowerVS service instance. network_name = \"ocp-net\" These set of variables specify the type of processor and physical system type to be used for the VMs. Change the default values according to your requirement. processor_type = \"shared\" #Can be shared or dedicated system_type = \"s922\" #Can be either s922 or e980 These set of variables specify the username and the SSH key to be used for accessing the bastion node. rhel_username = \"root\" public_key_file = \"data/id_rsa.pub\" private_key_file = \"data/id_rsa\" Please note that only OpenSSH formatted keys are supported. Refer to the following links for instructions on creating SSH key based on your platform. - Windows 10 - https://phoenixnap.com/kb/generate-ssh-key-windows-10 - Mac OSX - https://www.techrepublic.com/article/how-to-generate-ssh-keys-on-macos-mojave/ - Linux - https://www.siteground.com/kb/generate_ssh_key_in_linux/ Create the SSH key-pair and keep it under the data directory These set of variables specify the RHEL subscription details. This is sensitive data, and if you don't want to save it on disk, use environment variables RHEL_SUBS_USERNAME and RHEL_SUBS_PASSWORD and pass them to terraform apply command as shown in the Quickstart guide . If you are using CentOS as the bastion image, then leave these variables as-is. rhel_subscription_username = \"user@test.com\" rhel_subscription_password = \"mypassword\" This variable specifies the number of hardware threads (SMT) that's used for the bastion node. Default setting should be fine for majority of the use-cases. rhel_smt = 4","title":"OpenShift Cluster Details"},{"location":"ocp4-upi-powervs/docs/var.tfvars-doc/#openshift-installation-details","text":"These variables specify the URL for the OpenShift installer and client binaries. Change the URL to the specific pre-release version that you want to install on PowerVS. Reference link - https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview openshift_install_tarball = \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview/latest/openshift-install-linux.tar.gz\" openshift_client_tarball = \"https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp-dev-preview/latest/openshift-client-linux.tar.gz\" This variable specifies the OpenShift pull secret. This is available from the following link - https://cloud.redhat.com/openshift/install/power/user-provisioned Download the secret and copy it to data/pull-secret.txt . pull_secret_file = \"data/pull-secret.txt\" These variables specifies the OpenShift cluster domain details. Edit it as per your requirements. cluster_domain = \"ibm.com\" cluster_id_prefix = \"test-ocp\" cluster_id = \"\" Set the cluster_domain to nip.io , xip.io or sslip.io if you prefer using online wildcard domains. Default is ibm.com . The cluster_id_prefix should not be more than 8 characters. Nodes are pre-fixed with this value. Default value is test-ocp A random value will be used for cluster_id if not set. The total length of cluster_id_prefix . cluster_id should not exceed 14 characters.","title":"OpenShift Installation Details"},{"location":"ocp4-upi-powervs/docs/var.tfvars-doc/#misc-customizations","text":"These variables provides miscellaneous customizations. For common usage scenarios these are not required and should be left unchanged. The following variables can be used for disconnected install by using a local mirror registry on the bastion node. enable_local_registry = false #Set to true to enable usage of local registry for restricted network install. local_registry_image = \"docker.io/ibmcom/registry-ppc64le:2.6.2.5\" ocp_release_tag = \"4.4.9-ppc64le\" ocp_release_name = \"ocp-release\" This variable can be used for trying out custom OpenShift install image for development use. release_image_override = \"\" These variables specify the ansible playbooks that are used for OpenShift install and post-install customizations. helpernode_repo = \"https://github.com/RedHatOfficial/ocp4-helpernode\" helpernode_tag = \"5eab3db53976bb16be582f2edc2de02f7510050d\" install_playbook_repo = \"https://github.com/ocp-power-automation/ocp4-playbooks\" install_playbook_tag = \"02a598faa332aa2c3d53e8edd0e840440ff74bd5\" These variables can be used when debugging ansible playbooks installer_log_level = \"info\" ansible_extra_options = \"-v\" This variable specifies the external DNS servers to forward DNS queries that cannot be resolved locally. dns_forwarders = \"1.1.1.1; 9.9.9.9\" List of kernel arguments for the cluster nodes. Note that this will be applied after the cluster is installed and all the nodes are in Ready status. rhcos_kernel_options = [] Example 1 rhcos_kernel_options = [\"slub_max_order=0\",\"loglevel=7\"] These are NTP specific variables that are used for time-synchronization in the OpenShift cluster. chrony_config = true chrony_config_servers = [ {server = \"0.centos.pool.ntp.org\", options = \"iburst\"}, {server = \"1.centos.pool.ntp.org\", options = \"iburst\"} ] These set of variables are specific for cluster wide proxy configuration. Public internet access for the OpenShift cluster nodes is via Squid proxy deployed on the bastion. setup_squid_proxy = true If you have a separate proxy, and don't want to set the Squid proxy on bastion then use the following variables. setup_squid_proxy = false proxy = {server = \"hostname_or_ip\", port = \"3128\", user = \"pxuser\", password = \"pxpassword\"} Except server all other attributes are optional. Default port is 3128 with unauthenticated access. These variables specify details about NFS storage that is setup by default on the bastion server. storage_type = \"nfs\" volume_size = \"300\" # Value in GB volume_type = \"tier3\" volume_shareable = false If you need to attach additional data volumes to the OpenShift cluster nodes use the following variables. master_volume_size = \"500\" worker_volume_size = \"500\" The following variables are specific to upgrading an existing installation. upgrade_image = \"\" upgrade_pause_time = \"90\" upgrade_delay_time = \"600\" The following variables are specific to enable the connectivity between OCP nodes in PowerVS and IBM Cloud infrastructure over DirectLink. ibm_cloud_dl_endpoint_net_cidr = \"\" ibm_cloud_http_proxy = \"\"","title":"Misc Customizations"}]}